<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Layer Normalization &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html" />
    <link rel="prev" title="On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.html">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.html">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Layer Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08/a-fast-learning-algorithm-for-deep-belief-nets.html">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/27/sphinx-on-github-pages.html">Sphinx on GitHub Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Layer Normalization</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
<li><a class="reference internal" href="#notes">Notes</a><ul>
<li><a class="reference internal" href="#layer-normalization-analytic-gradients">Layer Normalization Analytic Gradients</a></li>
<li><a class="reference internal" href="#divisive-normalization">Divisive Normalization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html" title="Previous Chapter: On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; On Large-Batc...</span>
    </a>
  </li>
  <li>
    <a href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html" title="Next Chapter: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Batch Normali... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="layer-normalization">
<h1>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>Consider the <span class="math notranslate nohighlight">\(l^\textrm{th}\)</span> hidden layer in a deep feed-forward neural
network, and let <span class="math notranslate nohighlight">\(\mathbf{a}^l\)</span> be the vector representation of the summed
inputs to the neurons in that layer.  The summed inputs are computed through a
linear projection with the weight matrix <span class="math notranslate nohighlight">\(\mathbf{W}^l\)</span> and the bottom-up
inputs <span class="math notranslate nohighlight">\(\mathbf{h}^l\)</span> given as follows:</p>
<div class="math notranslate nohighlight">
\[a_i^l = {\mathbf{w}_i^l}^\top \mathbf{h}^l
\qquad \text{and} \qquad
\mathbf{h}^{l + 1} = \mathop{f}\left( a_i^l + b_i^l \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is an element-wise non-linear function, <span class="math notranslate nohighlight">\(\mathbf{w}_i^l\)</span>
is the incoming (row) weights to the <span class="math notranslate nohighlight">\(i^\textrm{th}\)</span> hidden units, and
<span class="math notranslate nohighlight">\(b_i^l\)</span> is the scalar bias parameter.</p>
<p>One of the challenges of deep learning is
<a class="reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html"><span class="doc">internal covariate shift</span></a>.
To reduce such undesirable covariate shift, batch normalization (BN) normalizes
the summed inputs to each hidden unit over the training cases:</p>
<div class="math notranslate nohighlight">
\[\newcommand{\E}[2][]{\left\langle #2 \right\rangle_{#1}}
\bar{a}_i^l = \frac{g_i^l}{\sigma_i^l} \left( a_i^l - \mu_i^l \right)
\qquad
\mu_i^l = \E[\mathbf{x} \sim P(\mathbf{x})]{a_i^l}
\qquad
\sigma_i^l =
  \sqrt{
    \E[\mathbf{x} \sim P(\mathbf{x})]{\left( a_i^l - \mu_i^l \right)^2}
  }\]</div>
<p>where <span class="math notranslate nohighlight">\(g_i^l\)</span> is a gain parameter scaling the normalized activation before
the non-linear activation function.  Here the expectation is over a mini-batch
instead of the entire training data distribution.</p>
<p>Feed-forward neural networks trained using batch normalization (BN) converge
faster even with a simple version of stochastic gradient descent.  Despite BN’s
simplicity, it requires running averages of the summed input statistics.  In
networks with fixed depth, the statistics can be stored separately for each
layer.  This does not scale to recurrent neural networks (RNNs) because
different statistics are needed for different time-steps.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p>The authors observed that changes in the output of one layer will tend to cause
highly correlated changes in the summed inputs to the next layer.  Thus, they
propose layer normalization (LN): fix the mean and the variance of the summed
inputs within each layer.  All the hidden units in a layer share the same
normalization terms</p>
<div class="math notranslate nohighlight">
\[\mu^l = \frac{1}{H} \sum_{i = 1}^H a_i^l
\qquad \text{and} \qquad
\sigma^l = \sqrt{\frac{1}{H} \sum_{i = 1}^H \left( a_i^l - \mu^l \right)^2}\]</div>
<p>where <span class="math notranslate nohighlight">\(H\)</span> denotes the number of hidden units, but different training cases
have different normalization terms.  In regards to RNNs, this formulation
depends only on the summed inputs to a layer at the current time-step and
requires only one set of gain and bias parameters shared over all time-steps.
The normalization terms make a layer invariant to any re-scaling of its summed
inputs, which prevents both exploding and vanishing gradients.</p>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>Even though the different normalization methods take on the form
<span class="math notranslate nohighlight">\(h_i = \mathop{f}\left( \frac{g_i}{\sigma_i} (a_i - \mu_i) + b_i \right)\)</span>,
each exhibits different invariance properties as derived in
<a class="reference internal" href="#norm-invariance"><span class="std std-numref">Table 1</span></a>.</p>
<table class="docutils align-default" id="norm-invariance">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Invariance Properties of Normalization Methods</span><a class="headerlink" href="#norm-invariance" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>Batch</p></th>
<th class="head"><p>Weight</p></th>
<th class="head"><p>Layer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>Weight matrix re-scaling</p></th>
<td><p>Invariant</p></td>
<td><p>Invariant</p></td>
<td><p>Invariant</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Weight matrix re-centering</p></th>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>Invariant</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Weight vector re-scaling</p></th>
<td><p>Invariant</p></td>
<td><p>Invariant</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Dataset re-scaling</p></th>
<td><p>Invariant</p></td>
<td><p>No</p></td>
<td><p>Invariant</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Dataset re-centering</p></th>
<td><p>Invariant</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Single training case re-scaling</p></th>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>Invariant</p></td>
</tr>
</tbody>
</table>
<p>The authors’ geometric analysis on the generalized linear model reveals how
normalization methods implicitly reduce learning rate and makes learning more
stable.  The Fisher information matrix shows that for the same parameter update
in the normalized model, the norm of the weight vector effectively controls the
learning rate for the weight vector.  Furthermore, learning the gain parameters
for the batch normalized and layer normalized models depends only on the
magnitude of the prediction error.  Hence, learning the magnitude of incoming
weights in the normalized model is more robust to the scaling of the input
and its parameters.</p>
<p>The authors applied LN to several RNN tasks such as image-sentence ranking,
question-answering, contextual language modelling, generative modelling,
handwriting sequence generation and MNIST classification.  The experimental
results indicate LN offers a per-iteration speedup across all metrics, converges
to the task’s best validation model, and even improves generalization over the
original model.</p>
<p>For MNIST classification, the authors only applied LN to the fully-connected
hidden layers that excludes the last softmax layer.  Compared to applying BN to
all layers, LN is robust to the batch-sizes and exhibits a faster training.
However, for convolutional layers, LN only offers a speedup over the baseline
model without normalization while BN outperforms all methods.  With
fully-connected layers, all the hidden units in a layer tend to make similar
contributions to the final prediction and re-centering and re-scaling the summed
inputs to a layer works well.  The assumption of similar contributions is not
valid for convolutional layers where the large number of the hidden units whose
receptive fields lie near the boundary of the image are rarely turned on and
thus have very different statistics from the rest of the hidden units within the
same layer.</p>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>If LN is augmented to be invariant to weight vector re-scaling, would it
overcome BN for CNNs?</p></li>
<li><p>How does the smoothing term affect the invariance properties of LN?</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The geometric analysis relies on the Fisher information matrix, but the
Adam optimizer does not even use the natural gradient.  To what extent does
this affect the analysis?</p></li>
<li><p>Does the initialization of the gains and bias matter?</p></li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Layer normalization is an effective and practical method that makes RNN training
stable while attaining faster convergence.  With the invariance properties in
mind, there is no reason to prefer weight normalization over LN
<a class="bibtex reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html#salimans2016weight" id="id1">[SK16]</a>.</p>
<p>The authors omitted too much experimental details on CNNs.  Their claim that LN
beats BN for the fully-connected layers needs more analysis.  They should have
presented the preliminary results on applying LN to CNN and what configurations
they tried.</p>
<p>Aside from <a class="reference internal" href="#norm-invariance"><span class="std std-numref">Table 1</span></a>, the geometric view of parameter spaces is
a very useful technique to comprehend.  The learnable parameters in a
statistical model form a smooth manifold that consists of all possible
input-output relations of the model.  For models whose output is a probability
distribution, a natural way to measure the separation of two points on this
manifold is the KL divergence between their model output distributions.  Under
the KL divergence metric, the parameter space is a Riemannian manifold.  The
curvature of a Riemannian manifold is entirely captured by its Riemannian
metric, which is well approximated using the
<a class="reference internal" href="../10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html"><span class="doc">Fisher information matrix</span></a>.</p>
<p>A generalization of batch and layer normalization is divisive normalization (DN)
<a class="bibtex reference internal" href="#ren2016normalizing" id="id2">[RLU+16]</a>.  DN modulates the neural activity by the activity of
a pool of neighboring neurons.  This phenomenon has been deemed as a canonical
computation of the brain.  Previous theoretical studies have outlined several
potential computational roles for DN such as distributed neural representations,
winner-take-all mechanisms, and contextual modulations in neural populations and
perception.  The experiments demonstrate that smoothing the normalizers and
adding a sparse <span class="math notranslate nohighlight">\(L^1\)</span> regularizer improves the accuracy of all three
normalizers.  Although the authors emphasized the appeal of DN, DN requires
tuning the size of the neighboring neurons.  Instead, the most interesting
implication is that localizing LN to individual regions cannot achieve
significant improvements.  This is extrapolated from the results of DN without
<span class="math notranslate nohighlight">\(L^1\)</span> regularization but with a near-zero smoothing term.  The authors
found that the smoothing term effectively modulates the rectified response to
vary from a linear to a highly saturated response.  Furthermore, the learnable
smoothing term tends to decrease with the depth of the network.</p>
<p>Another interpretation of layer normalization is group normalization (GN)
<a class="bibtex reference internal" href="#wu2018group" id="id3">[WH18]</a>.  GN is a layer that divides channels into groups and
normalizes the features within each group.  This proposal is motivated by a
well-accepted computational model in neuroscience that normalizes across the
cell responses with various receptive-field centers (covering the visual field)
and with various spatiotemporal frequency tunings.  The experiments defined a GN
with two groupings, and the results indicate GN is 1.2% better than LN for
visual recognition tasks.  However, the additional hyperparameter that picks
how many groupings makes GN’s significance questionable.</p>
</div>
<div class="section" id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="layer-normalization-analytic-gradients">
<h3>Layer Normalization Analytic Gradients<a class="headerlink" href="#layer-normalization-analytic-gradients" title="Permalink to this headline">¶</a></h3>
<p>Layer normalization for a particular activation consists of</p>
<div class="math notranslate nohighlight">
\[\begin{split}h_i &amp;\gets \mathop{f}\left( \frac{g_i}{\sigma} (a_i - \mu) + b_i \right)\\
\mu &amp;\gets \frac{1}{H} \sum_{i = 1}^H a_i\\
\sigma &amp;\gets \sqrt{\frac{1}{H} \sum_{i = 1}^H \left( a_i - \mu \right)^2}.\end{split}\]</div>
<p>To simplify notations, define <span class="math notranslate nohighlight">\(f' \triangleq
\mathop{f'}\left( \frac{g_i}{\sigma} (a_i - \mu) + b_i \right)\)</span>.  The gradients
of LN’s parameters with respect to some loss function <span class="math notranslate nohighlight">\(l\)</span> are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial g_i}
 &amp;= \frac{\partial l}{\partial h_i} \frac{\partial h_i}{\partial g_i}
  = \frac{\partial l}{\partial h_i} f' \frac{a_i - \mu}{\sigma}
\\\\
\frac{\partial l}{\partial b_i}
 &amp;= \frac{\partial l}{\partial h_i} \frac{\partial h_i}{\partial b_i}
  = \frac{\partial l}{\partial h_i} f'\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial a_i}
 &amp;= \frac{\partial l}{\partial h_i} \frac{\partial h_i}{\partial a_i}\\
 &amp;= \frac{\partial l}{\partial h_i} f'
    \left[
      g_i \frac{\partial}{\partial a_i}
        \left( a_i \sigma^{-1} - \mu \sigma^{-1} \right) +
      \frac{\partial}{\partial a_i} b_i
    \right]\\
 &amp;= \frac{\partial l}{\partial h_i} f' g_i
    \left[
        \sigma^{-1} + a_i \frac{\partial \sigma^{-1}}{\partial a_i} -
        \frac{\partial \mu}{\partial a_i} \sigma^{-1} -
        \mu \frac{\partial \sigma^{-1}}{\partial a_i}
    \right]\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \sigma^{-1}}{\partial a_i} =
\frac{-1}{2} \frac{1}{\sigma^3} \frac{2}{H} (a_i - \mu) =
\frac{-1}{H \sigma^3} (a_i - \mu)
\qquad \text{and} \qquad
\frac{\partial \mu}{\partial a_i} = H^{-1}.\]</div>
<p>Consider a smoothing term <span class="math notranslate nohighlight">\(s\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\sigma \gets
\sqrt{s^2 + \frac{1}{H} \sum_{i = 1}^H \left( a_i - \mu \right)^2}.\]</div>
<p>The corresponding gradient is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial s}
 &amp;= \frac{\partial l}{\partial h_i} \frac{\partial h_i}{\partial s}\\
 &amp;= \frac{\partial l}{\partial h_i} f' g_i (a_i - \mu)
    \frac{\partial \sigma^{-1}}{\partial s}\\
 &amp;= \frac{\partial l}{\partial h_i} f' g_i (a_i - \mu)
    \frac{-1}{2} \frac{1}{\sigma^3} 2s\\
 &amp;= \frac{\partial l}{\partial h_i} f' g_i (a_i - \mu) \frac{-s}{\sigma^3}.\end{split}\]</div>
</div>
<div class="section" id="divisive-normalization">
<h3>Divisive Normalization<a class="headerlink" href="#divisive-normalization" title="Permalink to this headline">¶</a></h3>
<p>DN models the response of a neuron <span class="math notranslate nohighlight">\(\tilde{z}_j\)</span> as a ratio between the
activity in a summation field <span class="math notranslate nohighlight">\(\mathcal{A}_j\)</span> and a norm-like function of
the suppression field <span class="math notranslate nohighlight">\(\mathcal{B}_j\)</span></p>
<div class="math notranslate nohighlight">
\[\tilde{z}_j =
\gamma \frac{
         \sum_{z_j \in \mathcal{A}_j} u_i z_i
       }{
         \left(
           \sigma^2 + \sum_{z_k \in \mathcal{B}_j} w_k z_k^p
         \right)^{1 / p}
       }\]</div>
<p>where <span class="math notranslate nohighlight">\(\{ u_i \}\)</span> are the summation weights, <span class="math notranslate nohighlight">\(\{ w_k \}\)</span> are the
suppression weights, and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is a smoothing term.</p>
<p>Consider the hidden input activation of one arbitrary layer in a deep neural
network as <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{N \times L}\)</span>.  Here <span class="math notranslate nohighlight">\(N\)</span> is
the mini-batch size, and in the case of a CNN, <span class="math notranslate nohighlight">\(L = H \times W \times C\)</span>.
For a RNN or fully-connected layers, <span class="math notranslate nohighlight">\(L\)</span> is the number of hidden units.</p>
<p>Each normalizer can be mapped to the following general form</p>
<div class="math notranslate nohighlight">
\[\begin{split}z_{n, j} &amp;= \sum_i w_{i, j} x_{n, i} + b_j\\
v_{n, j} &amp;= z_{n, j} - \E[\mathcal{A}_{n, j}]{z}\\
\tilde{z}_{n, j}
 &amp;= \frac{
      v_{n, j}
    }{
      \sqrt{\sigma^2 + \E[\mathcal{B}_{n, j}]{v^2}}
    }\end{split}\]</div>
<p>through different choices of range and normalizer bias in <a class="reference internal" href="#range-bias"><span class="std std-numref">Table 2</span></a>.</p>
<table class="docutils align-default" id="range-bias">
<caption><span class="caption-number">Table 2 </span><span class="caption-text">Different Parameterizations</span><a class="headerlink" href="#range-bias" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Summation Range</p></th>
<th class="head"><p>Suppression Range</p></th>
<th class="head"><p>Normalizer Bias</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BN</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{A}_{n, j} = \left\{ z_{m, j} \colon m \in [1, N], j \in [1, H] \times [1, W] \right\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B}_{n, j} = \left\{ v_{m, j} \colon m \in [1, N], j \in [1, H] \times [1, W] \right\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma = 0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>LN</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{A}_{n, j} = \left\{ z_{n, i} \colon i \in [1, L] \right\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B}_{n, j} = \left\{ v_{n, i} \colon i \in [1, L] \right\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma = 0\)</span></p></td>
</tr>
<tr class="row-even"><td><p>DN</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{A}_{n, j} = \left\{ z_{n, i} \colon d(i, j) \leq R_{\mathcal{A}} \right\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B}_{n, j} = \left\{ v_{n, i} \colon d(i, j) \leq R_{\mathcal{B}} \right\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma \geq 0\)</span></p></td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/12/14/layer-normalization-0"><dl class="citation">
<dt class="bibtex label" id="ba2016layer"><span class="brackets">BKH16</span></dt>
<dd><p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. <em>arXiv preprint arXiv:1607.06450</em>, 2016.</p>
</dd>
<dt class="bibtex label" id="ren2016normalizing"><span class="brackets"><a class="fn-backref" href="#id2">RLU+16</a></span></dt>
<dd><p>Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H Sinz, and Richard S Zemel. Normalizing the normalizers: comparing and extending network normalization schemes. <em>arXiv preprint arXiv:1611.04520</em>, 2016.</p>
</dd>
<dt class="bibtex label" id="wu2018group"><span class="brackets"><a class="fn-backref" href="#id3">WH18</a></span></dt>
<dd><p>Yuxin Wu and Kaiming He. Group normalization. In <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, 3–19. 2018.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/12/14/layer-normalization.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>