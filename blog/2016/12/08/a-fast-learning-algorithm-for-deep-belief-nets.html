<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>A Fast Learning Algorithm for Deep Belief Nets &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html" />
    <link rel="prev" title="Least-Squares Estimation of Transformation Parameters Between Two Point Sets" href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.html">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.html">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14/layer-normalization.html">Layer Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/27/sphinx-on-github-pages.html">Sphinx on GitHub Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">A Fast Learning Algorithm for Deep Belief Nets</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html" title="Previous Chapter: Least-Squares Estimation of Transformation Parameters Between Two Point Sets"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Least-Squares...</span>
    </a>
  </li>
  <li>
    <a href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html" title="Next Chapter: A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">A View of the... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="a-fast-learning-algorithm-for-deep-belief-nets">
<h1>A Fast Learning Algorithm for Deep Belief Nets<a class="headerlink" href="#a-fast-learning-algorithm-for-deep-belief-nets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>Learning is difficult in densely-connected, directed, multilayer belief nets
(DBN) because it is difficult to draw samples from the conditional distribution
of hidden activities when given a data vector.  This is manifested in the
phenomenon of explaining away: hidden states are independent in the prior but
dependent in the posterior.  Furthermore, even though belief networks can be
easily ancestral sampled, learning the model parameters for the first hidden
layer requires knowing the model parameters of higher layers.</p>
<p>The posterior distribution over the hidden variables
<span class="math notranslate nohighlight">\(p(\mathbf{h} \mid \mathbf{v})\)</span> is intractable except for mixture models
or linear models with additive Gaussian noise.  Markov Chain Monte Carlo methods
can be used to sample from the posterior if time is not a constraint.
Variational methods approximate the true posterior with a more tractable
distribution, and learning is guaranteed to improve a variational bound even
when the inference of the hidden states is done incorrectly.  However, the
approximations may be poor and variational learning requires all the parameters
to be learned together, which scales poorly as the number of parameters
increase.</p>
<p>One greedy procedure described in <a class="bibtex reference internal" href="#frean2009dbn" id="id1">[Fre]</a> to train a deep belief
network layer-by-layer is as follows:</p>
<ol class="arabic">
<li><p>Given the visible data layer <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> that is connected to the
hidden layer <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span>, maximize the likelihood of generating the
training data</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{v} \mid \mathbf{W}_1) =
\int p(\mathbf{v} \mid \mathbf{h}_1, \mathbf{W}_1)
     p(\mathbf{h}_1) d\mathbf{h}_1.\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span> is driven by an ephemeral hidden layer
<span class="math notranslate nohighlight">\(\mathbf{b}_1\)</span> that will serve as the <a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html"><span class="doc">bias layer</span></a>.</p></li>
<li><p>The <a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html"><span class="doc">EM</span></a>
algorithm can be used to optimize the weights
<span class="math notranslate nohighlight">\(\{ \mathbf{b}_1, \mathbf{W}_1 \}\)</span> to maximize the probability of
generating the data patterns.</p>
<ul>
<li><p>When the posterior is not available analytically, one must resort to
drawing samples from
<span class="math notranslate nohighlight">\(p(\mathbf{h}_1 \mid \mathbf{v}, \mathbf{W}_1)\)</span>.</p></li>
</ul>
</li>
<li><p>Averaging over the <span class="math notranslate nohighlight">\(N\)</span> training examples gives the aggregate
posterior <span class="math notranslate nohighlight">\(\frac{1}{N} \sum_{\mathbf{v} \in \mathcal{D}}
p(\mathbf{h}_1 \mid \mathbf{v}, \mathbf{W}_1)\)</span>.</p>
<ul>
<li><p>The weights <span class="math notranslate nohighlight">\(\mathbf{W}_1\)</span> end up at values that maximize the
likelihood of the data, given <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span> sampled from the
aggregated posterior distribution.</p></li>
<li><p>The bias weights <span class="math notranslate nohighlight">\(\mathbf{b}_1\)</span> end up at values that approximate
the aggregate posterior distribution.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Freeze the weights <span class="math notranslate nohighlight">\(\mathbf{W}_1\)</span>, replace <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span>’s
bias inputs by a second layer of weights <span class="math notranslate nohighlight">\(\mathbf{W}_2\)</span>.</p>
<ul class="simple">
<li><p>This stage discards the existing bias layer and its weights
<span class="math notranslate nohighlight">\(\mathbf{b}_1\)</span> to introduce the next hidden layer
<span class="math notranslate nohighlight">\(\mathbf{h}_2\)</span>.</p></li>
<li><p>A new ephemeral hidden bias layer <span class="math notranslate nohighlight">\(\mathbf{b}_2\)</span> will drive
<span class="math notranslate nohighlight">\(\mathbf{h}_2\)</span>.</p></li>
</ul>
</li>
<li><p>Given <span class="math notranslate nohighlight">\(\mathbf{W}_1\)</span> and treating <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span> as the data
(visible) layer, proceed to maximize the likelihood</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{h}_1 \mid \mathbf{W}_2) =
\int p(\mathbf{h}_1 \mid \mathbf{h}_2, \mathbf{W}_2)
     p(\mathbf{h}_2) d\mathbf{h}_2.\]</div>
<ul class="simple">
<li><p>For each visible pattern <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, the freezed weights are used
to sample from <span class="math notranslate nohighlight">\(p(\mathbf{h}_1 \mid \mathbf{v}, \mathbf{W}_1)\)</span>.</p></li>
<li><p>This will make the <span class="math notranslate nohighlight">\(\mathbf{h}_2\)</span> learn the aggregate posterior
distribution over <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span>.</p></li>
</ul>
</li>
<li><p>Proceed recursively until all the layers are learned.</p></li>
</ol>
<p>This greedy procedure does not work well at all.  Recall that if
<span class="math notranslate nohighlight">\(p(\mathbf{v})\)</span> is factorial i.e. <span class="math notranslate nohighlight">\(p(\mathbf{v}) = \prod_i p(v_i)\)</span>,
there is no point in having hidden units in the generative model because a model
that has hidden units can do no better than a model with just bias inputs to the
visible units.</p>
<p>In a directed belief network with one hidden layer connected by weights
<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, the prior <span class="math notranslate nohighlight">\(p(\mathbf{h})\)</span> is factorial, and the
posterior <span class="math notranslate nohighlight">\(p(\mathbf{h} \mid \mathbf{v})\)</span> is non-factorial due to
explaining away.  Learning weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> with EM will result in
features that are independent in the prior, which tends to make the aggregate
posterior as factorial as possible.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p>The authors observe that when trying to improve</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{v}) =
\int p(\mathbf{h}) p(\mathbf{v} \mid \mathbf{h}) d\mathbf{h},\]</div>
<p>solely focusing on the prior can indirectly refine <span class="math notranslate nohighlight">\(p(\mathbf{v})\)</span>.  To
improve the prior, a better model of the aggregate posterior distribution is
desirable.</p>
<p>One way to enhance the aggregate posterior distribution is to make drawing
samples easier by removing the effects of explaining away.  This can be done by
restricting the likelihood and prior to the <a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html"><span class="doc">exponential family</span></a>.
As shown in the appendix, the exponential family always have a (complementary)
prior that makes the posterior exactly factorize.</p>
<p>In order to create a complementary prior for each greedily learned layer, first
recall how the first layer of a directed belief network is learned.  Let
<span class="math notranslate nohighlight">\(\mathbf{v} = \mathbf{x}^{(0)}, \mathbf{h}_1 = \mathbf{y}^{(0)},
\mathbf{x}^{(1)}, \mathbf{y}^{(1)}, \ldots,
\mathbf{x}^{(\infty)}, \mathbf{y}^{(\infty)}\)</span> be an infinite sequence (stack) of
variables where <span class="math notranslate nohighlight">\(p( \mathbf{x}^{(0)} \mid \mathbf{y}^{(0)})\)</span>
and <span class="math notranslate nohighlight">\(p( \mathbf{y}^{(0)} \mid \mathbf{x}^{(0)})\)</span> factorizes.
<span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}^{(i)}\)</span> represent a sequence of
successively deeper layers with tied weights, i.e., the parameters defining the
conditional distributions between layers are shared.  This sequence can be
interpreted as unrolling the Gibbs sampling scheme in space such that each
parallel update of the variables defines the states of a separate layer in the
graph.  The infinite stack of directed graphs with tied weights has been proven
(in the appendix) to produce a factorial posterior but non-factorial prior,
assuming first-order Markovian dependency and detailed balance holds.  The
Markov chain’s detailed balance property reveals that a top-down pass of the
directed belief network is equivalent to letting a <a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html"><span class="doc">RBM</span></a>
settle to equilibrium <a class="bibtex reference internal" href="#faulkner2010dbnrl" id="id2">[Fau]</a>.  Since a RBM has a factorial
posterior but non-factorial prior, the learning process does not force the
aggregate posterior to be factorial.</p>
<p>The model above <span class="math notranslate nohighlight">\(\mathbf{y}^{(0)}\)</span> implements a complementary prior.
However, once the next hidden layer changes the tied weights, the priors for the
already learned lower layers cease to be complementary.  Consequently, the
posterior distributions in the lower layers are no longer factorial.  This
approximation, which assumes higher layers exist but have tied weights
initially, is more reasonable than one that ignores the higher layers.</p>
<p>The authors assert that if image-label pairs were generated as
<em>stuff -&gt; image -&gt; label</em>, it would make sense to learn a mapping from images to
labels.  However, if image-label pairs are generated as
<em>image &lt;- stuff -&gt; label</em>, it makes economic sense to do unsupervised learning
on the image followed by supervised learning with the labels
<a class="bibtex reference internal" href="#hintonucldbn" id="id3">[Hinb]</a>.</p>
<dl>
<dt>Unsupervised Pre-training</dt><dd><p>Unlabeled data can be used to discover good features as follows:</p>
<ol class="arabic simple">
<li><p>Given the visible data layer <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> that is connected to the
hidden layer <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span>, proceed to learn the weights
<span class="math notranslate nohighlight">\(\mathbf{W}\)</span> in the same manner as a RBM.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html"><span class="doc">Contrastive divergence</span></a>
can be used instead of alternating Gibbs sampling.</p></li>
<li><p>The typical application of contrastive divergence fails for deep,
multilayer networks with different weights at each layer because these
networks take far too long even to reach conditional equilibrium.</p></li>
</ul>
</li>
<li><p>Freeze (i.e. make a copy of) <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and denote it as
<span class="math notranslate nohighlight">\(\mathbf{W}_1\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_1\)</span> is typically described as being untied from the
tied weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.</p></li>
</ul>
</li>
<li><p>Given <span class="math notranslate nohighlight">\(\mathbf{W}_1\)</span> and treating
<span class="math notranslate nohighlight">\(\mathbf{h}_1 \sim p(\mathbf{h}_1 \mid \mathbf{v}, \mathbf{W}_1)\)</span> as
the data (visible) layer, train the next hidden layer <span class="math notranslate nohighlight">\(\mathbf{h}_2\)</span>
with a RBM.</p>
<ul class="simple">
<li><p>This will make <span class="math notranslate nohighlight">\(\mathbf{h}_2\)</span> refine <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> to better
model the aggregated posterior distribution.</p></li>
</ul>
</li>
<li><p>Proceed recursively until all the layers are learned.</p></li>
</ol>
</dd>
<dt>Supervised Fine-tuning</dt><dd><p>The pre-training stage uses the frozen weights as recognition weights and
generative weights.  As a result, neither the weights nor the inference
procedure are optimal for the lower layers due to underfitting.</p>
<p>To refine the weights, the <a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html"><span class="doc">contrastive wake-sleep</span></a>
algorithm is one way to untie the recognition weights from the generative
ones.  The wake-phase is still the same: perform a bottom-up pass, starting
with a pattern from the training set, and use the delta rule to increase the
likelihood of the generative model.  The goal of the sleep-phase is still the
same: perform a top-down pass and use the delta rule to increase the
likelihood of the recognition model.  However, instead of starting from an
equilibrium sample from the top-level associative memory, the associative
memory is initialized by a bottom-up pass followed by contrastive divergence.
This ensures that the recognition weights capture representations that
resemble real data and eliminate the problem of mode averaging.</p>
<p>Note that a DBN is not a multilayer Boltzmann machine
<span class="bibtex" id="id4">[salakhutdinov2009deep]</span>: it has undirected connections between its top
two layers and downward directed connections between all its lower layers.
After the weights are untied, the whole probabilistic framework can be
discarded.  Only the generative weights in the bottom-up direction are used to
initialize all the feature detecting layers of a deterministic feedforward
deep neural network (<a class="reference internal" href="../02/learning-internal-representations-by-error-propagation.html"><span class="doc">DNN</span></a>).
The network can then be augmented with the desired final output layer (e.g.
softmax) and trained discriminatively via backpropagation.</p>
<p>The proposed consecutive phases of fine-tuning only modifies the features
slightly to get the category boundaries right.  It does not need to discover
features.  The resulting network is called DBN-DNN to differentiate its
training regime from typical DNNs <a class="bibtex reference internal" href="#hinton2012deep" id="id5">[HDY+12]</a>.</p>
</dd>
</dl>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>The authors reasoned at a high level that the RBM learning rule is the same as
the maximum likelihood learning rule for the infinite logistic belief network
with tied weights.</p>
<p>Recall that each pair of layers in the infinite belief network with tied weights
takes the form of</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}, \mathbf{y}) =
Z^{-1} \exp\left(
  \sum_{i, j} \Psi_{i, j}(x_i, y_j) +
  \sum_i \gamma_i(x_i) +
  \sum_j \alpha_j(y_j)
\right) =
Z^{-1} \exp E(\mathbf{x}, \mathbf{y})\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[Z = \sum_{\mathbf{x}', \mathbf{y}'} \exp E(\mathbf{x}', \mathbf{y}').\]</div>
<p>To simplify notation from here on, define <span class="math notranslate nohighlight">\(E^{(i, j)} =
\mathop{E}\left( \mathbf{x}^{(i)}, \mathbf{y}^{(j)} \right)\)</span>.</p>
<p>The derivatives of the log-probability of each layer are given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial w_{ij}} \log p(\mathbf{x}^{(i)})
 &amp;= \frac{\partial}{\partial w_{ij}} \left(
      \log \sum_{\mathbf{y}^{(i)}} \exp E^{(i, i)} -
      \log Z
    \right)\\
 &amp;= \left( \sum_{\mathbf{y}^{(i)}} \exp E^{(i, i)} \right)^{-1}
      \sum_{\mathbf{y}^{(i)}}
        \exp \left\{ E^{(i, i)} \right\}
        \frac{\partial E^{(i, i)}}{\partial w_{ij}} -
    Z^{-1} \sum_{\mathbf{x}', \mathbf{y}^{(i)}}
      \exp \left\{ E^{(', i)} \right\}
      \frac{\partial E^{(', i)}}{\partial w_{ij}}\\
 &amp;= \sum_{\mathbf{y}^{(i)}}
      p(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
      \frac{\partial E^{(i, i)}}{\partial w_{ij}} -
    \sum_{\mathbf{x}', \mathbf{y}^{(i)}}
      p(\mathbf{x}', \mathbf{y}^{(i)})
      \frac{\partial E^{(', i)}}{\partial w_{ij}}\\
 &amp;= \left\langle
      \frac{\partial E^{(i, i)}}{\partial w_{ij}}
    \right\rangle_{p(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})} -
    \left\langle
      \frac{\partial E^{(', i)}}{\partial w_{ij}}
    \right\rangle_{p(\mathbf{x}', \mathbf{y}^{(i)})}
    &amp; \quad &amp; p(\mathbf{x}', \mathbf{y}^{(i)}) =
              p(\mathbf{x}' \mid \mathbf{y}^{(i)}) p(\mathbf{y}^{(i)})\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial w_{ij}} \log p(\mathbf{y}^{(i)})
 &amp;= \frac{\partial}{\partial w_{ij}} \left(
      \log \sum_{\mathbf{x}^{(i + 1)}} \exp E^{(i + 1, i)} -
      \log Z
    \right)\\
 &amp;= \left\langle
      \frac{\partial E^{(i, i)}}{\partial w_{ij}}
    \right\rangle_{p(\mathbf{x}^{(i + 1)} \mid \mathbf{y}^{(i)})} -
    \left\langle
      \frac{\partial E^{(i + 1, i')}}{\partial w_{ij}}
    \right\rangle_{p(\mathbf{x}^{(i + 1)}, \mathbf{y}')}
    &amp; \quad &amp; p(\mathbf{x}^{(i + 1)}, \mathbf{y}') =
              p(\mathbf{y}' \mid \mathbf{x}^{(i + 1)})
                p(\mathbf{x}^{(i + 1)}).\end{split}\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(\langle \cdot \rangle\)</span> can be estimated via <a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html"><span class="doc">Gibbs sampling</span></a>.
Notice how <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> can be either <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> or
<span class="math notranslate nohighlight">\(\mathbf{x}^{(i + 1)}\)</span> because the weights are tied (see Figure 3) and
the generative process converges to the stationary distribution of the Markov
chain <a class="bibtex reference internal" href="#hinton2007nipsdbn" id="id6">[Hina]</a>.  Likewise, <span class="math notranslate nohighlight">\(\mathbf{y}'\)</span> can be either
<span class="math notranslate nohighlight">\(\mathbf{y}^{(i)}\)</span> or <span class="math notranslate nohighlight">\(\mathbf{y}^{(i + 1)}\)</span>.  Note that the bias
terms in the <a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html"><span class="doc">conditional probability distributions</span></a>
can be rolled into <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.  Since the weights are tied, the full
derivative for a generative weight is obtained by</p>
<div class="math notranslate nohighlight">
\[\sum_{i = 0}^{\infty - 1}
  \frac{\partial}{\partial w_{ij}} \left(
    \log p(\mathbf{x}^{(i)}) + \log p(\mathbf{y}^{(i)})
  \right) =
\left\langle
  \frac{\partial E^{(0, 0)}}{\partial w_{ij}}
\right\rangle_{p(\mathbf{y}^{(0)} \mid \mathbf{x}^{(0)})} -
  \left\langle
    \frac{\partial E^{(\infty, \infty)}}{\partial w_{ij}}
  \right\rangle_{p(\mathbf{x}^{(\infty)}, \mathbf{y}^{(\infty)})}.\]</div>
<p>Another justification for the greedy learning algorithm is that the incorrect
inference procedure gives a <a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html"><span class="doc">variational lower bound</span></a>
on the log probability of the data.  The higher layers learn a prior that is
closer to the aggregated posterior distribution of the first hidden layer.</p>
<p>The greedy layer-by-layer training enabled the network to find a solution that
achieved a competitive low error rate on MNIST without the use of data
augmentation and domain-specific tricks such as weight-sharing and subsampling.</p>
<p>Unlike existing discriminative models, the generative model after learning
<span class="math notranslate nohighlight">\(n\)</span> layers can generate data in the same manner as a
<a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html"><span class="doc">Helmholtz machine</span></a>:</p>
<ol class="arabic simple">
<li><p>Get an equilibrium sample from the top-level RBM by performing alternating
Gibbs sampling between the top-level layer and the penultimate layer.</p></li>
<li><p>Perform a top-down pass to get states for all the other layers.</p></li>
</ol>
<p>This additional insight into the network makes it possible to interpret the
non-linear, distributed representations of the hidden layers.  The samples drawn
from the learned generative model were representative of the real data.</p>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>How would one interpret the non-linear, distributed presentations when dealing
with abstract concepts such as mathematics?</p></li>
<li><p>How to quantify the suboptimality of assuming the likelihood factorizes?</p></li>
<li><p>Since a single layer feedforward neural network is a
<a class="reference internal" href="../../11/05/multilayer-feedforward-networks-are-universal-approximators.html"><span class="doc">universal approximator</span></a>,
would greedy learning lose information?  Would it make recovering the original
information much more difficult?</p></li>
<li><p>How to validate the learned concepts of each layer?</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The test cases that the network got wrong were pretty easy.  Were these
scenarios not covered by the training set?</p></li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Supervised learning of a mapping between data and labels is inefficient due to
the limited supply of labels.  Unsupervised feature learning is one way to make
use of unlabeled data.</p>
<p>A brief introductory overview can be found in <a class="bibtex reference internal" href="#hinton2006reducing" id="id7">[HS06]</a>, but
reading it is unnecessary if one has or is going to read <a class="bibtex reference internal" href="#hinton2006fast" id="id8">[HOT06]</a>.
A thorough understanding <a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html"><span class="doc">EFHs</span></a>
will make this paper easier to understand.</p>
<p>The idea of fine-tuning appeared previously as graph transformer networks
<a class="bibtex reference internal" href="#bottou1997global" id="id9">[BBLC97]</a>.  The main takeaway is to connect pipelined components
together as a feed-forward network of differentiable modules and perform
backpropagation to tune the parameters of each component simultaneously.</p>
<p>The proof techniques and concept of complementary priors look to be very useful
for future directions.  The paper would be even better if not for the confusing
summary of <a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html"><span class="doc">RBM contrastive divergence</span></a>.
The most interesting insight is that ancestral sampling on an infinitely deep
belief network with tied weights is equivalent to block Gibbs sampling on a RBM.</p>
<p>As an aside, do not bother reading <a class="bibtex reference internal" href="#bengio2007greedy" id="id10">[BLPL07]</a>.  It offers zero new
insights compared to the other papers.  Similarly, <a class="bibtex reference internal" href="#erhan2010does" id="id11">[EBC+10]</a> is too
verbose.  In essence, the paper goes over a set of experiments suggesting
unsupervised pre-training may be classified as an
initialization-as-regularization strategy.  The experiments’ results can be
summarized as verifying the consistent usefulness of unsupervised pre-training.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/12/08/a-fast-learning-algorithm-for-deep-belief-nets-0"><dl class="citation">
<dt class="bibtex label" id="bengio2007greedy"><span class="brackets"><a class="fn-backref" href="#id10">BLPL07</a></span></dt>
<dd><p>Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In <em>Advances in neural information processing systems</em>, 153–160. 2007.</p>
</dd>
<dt class="bibtex label" id="bottou1997global"><span class="brackets"><a class="fn-backref" href="#id9">BBLC97</a></span></dt>
<dd><p>Léon Bottou, Yoshua Bengio, and Yann Le Cun. Global training of document processing systems using graph transformer networks. In <em>Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on</em>, 489–494. IEEE, 1997.</p>
</dd>
<dt class="bibtex label" id="erhan2010does"><span class="brackets"><a class="fn-backref" href="#id11">EBC+10</a></span></dt>
<dd><p>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? <em>Journal of Machine Learning Research</em>, 11(Feb):625–660, 2010.</p>
</dd>
<dt class="bibtex label" id="faulkner2010dbnrl"><span class="brackets"><a class="fn-backref" href="#id2">Fau</a></span></dt>
<dd><p>Ryan Faulkner. Deep belief nets in reinforcement learning. <span><a class="reference external" href="#"></a></span>http://www.cs.mcgill.ca/ rfaulk/presentationDBN_final.pdf. Accessed slides 16 and 17: 2017-09-01.</p>
</dd>
<dt class="bibtex label" id="frean2009dbn"><span class="brackets"><a class="fn-backref" href="#id1">Fre</a></span></dt>
<dd><p>Marcus Frean. Aciss’09 tutorial on deep belief nets. <span><a class="reference external" href="#"></a></span>http://goanna.cs.rmit.edu.au/ xiaodong/aciss09/tute-slides/ACISS-frean-tutorial-2x2.pdf. Accessed: 2017-09-06.</p>
</dd>
<dt class="bibtex label" id="hinton2007nipsdbn"><span class="brackets"><a class="fn-backref" href="#id6">Hina</a></span></dt>
<dd><p>Geoffrey Hinton. 2007 nips tutorial on: deep belief nets. <span><a class="reference external" href="#"></a></span>https://www.cs.toronto.edu/ hinton/nipstutorial/nipstut3.pdf. Accessed slides 48 and 49: 2017-09-06.</p>
</dd>
<dt class="bibtex label" id="hintonucldbn"><span class="brackets"><a class="fn-backref" href="#id3">Hinb</a></span></dt>
<dd><p>Geoffrey Hinton. Ucl tutorial on deep belief nets. <span><a class="reference external" href="#"></a></span>https://www.cs.toronto.edu/ hinton/ucltutorial.pdf. Accessed: 2017-09-06.</p>
</dd>
<dt class="bibtex label" id="hinton2012deep"><span class="brackets"><a class="fn-backref" href="#id5">HDY+12</a></span></dt>
<dd><p>Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, and others. Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. <em>IEEE Signal Processing Magazine</em>, 29(6):82–97, 2012.</p>
</dd>
<dt class="bibtex label" id="hinton2006fast"><span class="brackets"><a class="fn-backref" href="#id8">HOT06</a></span></dt>
<dd><p>Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. <em>Neural computation</em>, 18(7):1527–1554, 2006.</p>
</dd>
<dt class="bibtex label" id="hinton2006reducing"><span class="brackets"><a class="fn-backref" href="#id7">HS06</a></span></dt>
<dd><p>Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. <em>Science</em>, 313(5786):504–507, 2006.</p>
</dd>
</dl>
</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/12/08/a-fast-learning-algorithm-for-deep-belief-nets.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>