<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html" />
    <link rel="prev" title="Understanding Deep Learning Requires Rethinking Generalization" href="../17/understanding-deep-learning-requires-rethinking-generalization.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.html">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14/layer-normalization.html">Layer Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08/a-fast-learning-algorithm-for-deep-belief-nets.html">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/27/sphinx-on-github-pages.html">Sphinx on GitHub Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../17/understanding-deep-learning-requires-rethinking-generalization.html" title="Previous Chapter: Understanding Deep Learning Requires Rethinking Generalization"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Understanding...</span>
    </a>
  </li>
  <li>
    <a href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html" title="Next Chapter: On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">On Large-Batc... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="accurate-large-minibatch-sgd-training-imagenet-in-1-hour">
<h1>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour<a class="headerlink" href="#accurate-large-minibatch-sgd-training-imagenet-in-1-hour" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>The supervised learning regime consists of minimizing a loss <span class="math notranslate nohighlight">\(L(w)\)</span> of the
form</p>
<div class="math notranslate nohighlight">
\[L(w) =
\frac{1}{\left\vert \mathcal{X} \right\vert}
  \sum_{x \in \mathcal{X}} l(x, w).\]</div>
<p>Here <span class="math notranslate nohighlight">\(w\)</span> are the network weights, <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is a labeled
training set, <span class="math notranslate nohighlight">\(l(x, w)\)</span> is the loss computed from samples
<span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span> and their labels <span class="math notranslate nohighlight">\(y\)</span>.  A typical method to
minimize this function is minibatch stochastic gradient descent (SGD):</p>
<div class="math notranslate nohighlight">
\[w_{t + 1} = w_t - \eta \frac{1}{n} \sum_{x \in \mathcal{B}} \nabla l(w, w_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is a minibatch sampled from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>,
<span class="math notranslate nohighlight">\(n = \left\vert \mathcal{B} \right\vert\)</span> is the minibatch size,
<span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate, and <span class="math notranslate nohighlight">\(t\)</span> is the iteration index.  After
<span class="math notranslate nohighlight">\(k\)</span> iterations the weights become</p>
<div class="math notranslate nohighlight">
\[w_{t + k} =
w_t + \eta \frac{1}{n} \sum_{j &lt; k}
    \sum_{x \in \mathcal{B}_j} \nabla l(x, w_{t + j}).\]</div>
<p>A brute-force way to speed up training is to divide the batch over many workers.
Consequently, a larger batch size is needed in order to fully utilize each
worker.  Taking a single step with the large minibatch <span class="math notranslate nohighlight">\(\bigcup_j B_j\)</span> of
size <span class="math notranslate nohighlight">\(kn\)</span> and learning rate <span class="math notranslate nohighlight">\(\hat{\eta}\)</span> yields</p>
<div class="math notranslate nohighlight">
\[\hat{w}_{t + 1} =
w_t + \hat{\eta} \frac{1}{kn} \sum_{j &lt; k}
    \sum_{x \in \mathcal{B}_j} \nabla l(x, w_t).\]</div>
<p>Assuming that <span class="math notranslate nohighlight">\(\nabla l(x, w_t) \approx \nabla l(x, w_{t + j})\)</span> for
<span class="math notranslate nohighlight">\(j &lt; k\)</span>, setting <span class="math notranslate nohighlight">\(\hat{\eta} = kn\)</span> would yield
<span class="math notranslate nohighlight">\(\hat{w}_{t + 1} \approx w_{t + k}\)</span>.  The above interpretation gives the
intuition behind the <a class="reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html"><span class="doc">linear scaling rule</span></a>:
when the minibatch size is multiplied by <span class="math notranslate nohighlight">\(k\)</span>, multiply the learning rate
by <span class="math notranslate nohighlight">\(k\)</span> while keeping the number of epochs and other hyperparameters
unchanged.</p>
<p>The linear scaling rule is also applicable when using batch normalization (BN)
with large minibatches.  Recall that <a class="reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html"><span class="doc">BN</span></a>
breaks the independence of each sample’s loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L(w)
 &amp;= \frac{1}{\left\vert \mathcal{X}^n \right\vert}
    \sum_{\mathcal{B} \in \mathcal{X}^n} L(\mathcal{B}, w)\\
 &amp;= \frac{1}{\left\vert \mathcal{X}^n \right\vert}
    \sum_{\mathcal{B} \in \mathcal{X}^n}
      \frac{1}{n} \sum_{x \in \mathcal{B}} l_\mathcal{B}(x, w).\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\mathcal{X}^n\)</span> denotes all distinct subsets with <span class="math notranslate nohighlight">\(n\)</span> elements
in the power set of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.  When viewing <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> as a
single sample in <span class="math notranslate nohighlight">\(\mathcal{X}^n\)</span>, the loss of each <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is
computed independently.  Since changing the per-worker minibatch sample size
<span class="math notranslate nohighlight">\(n\)</span> alters the underlying loss function <span class="math notranslate nohighlight">\(L\)</span>, BN statistics should
not be aggregated across all workers.  Therefore, the case of large minibatch
training with BN is analogous to the foregoing per-sample loss formulation: a
total minibatch size of <span class="math notranslate nohighlight">\(kn\)</span> can be viewed as a minibatch of <span class="math notranslate nohighlight">\(k\)</span>
samples with each sample <span class="math notranslate nohighlight">\(\mathcal{B}_j\)</span> independently selected from
<span class="math notranslate nohighlight">\(\mathcal{X}^n\)</span>.  The aforementioned assumption now becomes
<span class="math notranslate nohighlight">\(\nabla L(\mathcal{B}_j, w_t) \approx \nabla L(\mathcal{B}_j, w_{t + j})\)</span>.</p>
<p>In practice, the above assumptions do not hold in the initial training epochs
when the network is changing rapidly and <a class="reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html"><span class="doc">negatively impacts the model accuracy</span></a>.
One strategy to mitigate this issue is a constant warmup phase: use a less
aggressive constant learning rate for the first few training epochs.  However,
given a large enough <span class="math notranslate nohighlight">\(k\)</span>, this constant warmup is not sufficient to solve
the optimization problem.  Furthermore, the abrupt transition from the low
learning rate causes the training error to spike.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p>The authors propose an alternative warmup that gradually ramps up the learning
rate from a small to a large value.  With a batch size of <span class="math notranslate nohighlight">\(kn\)</span>, the
authors start from a learning rate of <span class="math notranslate nohighlight">\(\eta\)</span> and increment it by a
constant amount at each iteration such that it reaches
<span class="math notranslate nohighlight">\(\hat{\eta} = k \eta\)</span> after <span class="math notranslate nohighlight">\(5\)</span> epochs.  After the warmup phase, the
training uses the original learning rate schedule.</p>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>The authors verified on the 1000-way ImageNet classification that the linear
scaling warmup yields training and validation error curves that closely match
the small minibatch baseline.  They used a single random shuffling of the
training data (per epoch) that is divided amongst all <span class="math notranslate nohighlight">\(k\)</span> workers.  The
per-worker loss is normalized using <span class="math notranslate nohighlight">\(\frac{1}{kn}\)</span> because allreduce
performs summing, not averaging.</p>
<p>Their infrastructure consists of 32 servers, each equipped with eight P100 GPUs
and 3.2 TB of NVMe.  Since backpropagation on a P100 takes 120ms for ResNet-50
(~15 Gbps peak bandwidth), interserver communication uses ConnectX-4 50 Gbps
Ethernet network card with Wedge100 Ethernet switches.  All models were trained
for 90 epochs (one hour) using a local batch size of <span class="math notranslate nohighlight">\(n = 32\)</span>.  The
reference learning rate <span class="math notranslate nohighlight">\(\eta = 0.1 \frac{kn}{256}\)</span> was reduced by
<span class="math notranslate nohighlight">\(\frac{1}{10}\)</span> at the <span class="math notranslate nohighlight">\({30}^\text{th}\)</span>, <span class="math notranslate nohighlight">\({60}^\text{th}\)</span>, and
<span class="math notranslate nohighlight">\({80}^\text{th}\)</span> epoch.  For BN layers, they initialized
<span class="math notranslate nohighlight">\(\gamma = 1\)</span> except for each residual block’s last BN where
<span class="math notranslate nohighlight">\(\gamma = 0\)</span>.  This modification forces the forward/backward signal to
initially propagate through the identity shortcut of ResNet, which seems to ease
optimization at the start of training for both small and large minibatches.</p>
<p>The comparison between no warmup and gradual warmup suggests that large
minibatch sizes are challenged by optimization difficulties in early training.
The authors observed no generalization issues when transferring across datasets
(from ImageNet to COCO) and across tasks (from classification to
detection/segmentation) using models trained with large minibatches.  However,
when <span class="math notranslate nohighlight">\(kn &gt; 8192\)</span>, the training and validation error curves start to
diverge towards lower accuracy.</p>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>How does the accuracy change when batch normalization is replaced with
<a class="reference internal" href="../14/layer-normalization.html"><span class="doc">smoothed layer normalization</span></a>?</p></li>
<li><p>Why is large batch training sensitive to the initialization of the weights?</p></li>
<li><p>Does the three phase generalization dynamics still hold when training neural
networks from scratch?</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Scaling the cross-entropy loss is not equivalent to scaling the learning rate,
but why would one want to boost the cross-entropy loss?</p></li>
<li><p>A momentum correction factor is needed when the learning rate delays the
scaling of the momentum decay factor by one iteration.  Why is the delayed
scaling formulation useful?</p></li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Optimization and generalization of large minibatch training using gradual warmup
with linear scaling matches that of small minibatch training.  Incidentally, the
training error curves can be used as a reliable proxy for success well before
training finishes.</p>
<p>Although the gradual warmup may seem like an arbitrary hack, there is precedent
in doing so.  <a class="bibtex reference internal" href="#wang1994optimal" id="id1">[WVJ94]</a> demonstrated that there are in general
three distinct phases of learning.  At the beginning, the network has
hardly learned anything and is still very biased.  This phase of training is
dominated by the approximation error.  As training continues, the approximation
error will decrease at the cost of increasing the complexity error.  If the
network is trained long enough, the complexity error will dominate, which
implies that early stopping is a mechanism to detect when phase three starts.
Unfortunately, this analysis is for networks where only the output weights are
being trained.</p>
<p>The authors assert that optimization difficulty is the main issue with large
minibatch training, rather than poor generalization.  They should have evaluated
the sharpness of the local optimums because their results only hold when
<span class="math notranslate nohighlight">\(kn \leq 8\text{K}\)</span> and <a class="bibtex reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html#keskar2016large" id="id2">[KMN+16]</a> states that simply
increasing the batch size will lead to a solution that has poor generalization.</p>
<p>One point that deserves mention is that the square root scaling rule that was
justified theoretically in <a class="bibtex reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html#krizhevsky2014one" id="id3">[Kri14]</a> works poorly in practice.</p>
<p>The implementation details of gradient aggregation are very useful for future
system designs.  The authors claimed that collective communication was not a
bottleneck for their allreduce implementation.  Their scheme consists of three
phases:</p>
<ol class="arabic simple">
<li><p>For each server, buffers from its GPUs are summed into a single buffer.</p></li>
<li><p>The resulting buffers are shared and summed across all servers.</p></li>
<li><p>The final results are broadcasted to each GPU.</p></li>
</ol>
<p>Phases (1) and (3) are handled by NCCL while the interserver allreduce uses
the recursive halving and doubling algorithm, instead of the bucket (ring)
algorithm.  Given a buffer of <span class="math notranslate nohighlight">\(b\)</span> bytes and a cluster of <span class="math notranslate nohighlight">\(p\)</span>
servers, both sends and receives <span class="math notranslate nohighlight">\(2 \frac{p - 1}{p} b\)</span> bytes of data.  The
former takes <span class="math notranslate nohighlight">\(2 \log_2 p\)</span> communication steps while the latter requires
<span class="math notranslate nohighlight">\(2 (p - 1)\)</span> steps.  A generalized version of the halving/doubling
algorithm that supports non-power-of-two cluster size is the binary blocks
algorithm: servers are partitioned into power-of-two blocks and two additional
communication steps are used, one immediately after the intrablock
reduce-scatter and one before the intrablock allgather.</p>
<p>A subsequent work <a class="bibtex reference internal" href="#you2017lars" id="id4">[YGG17]</a> demonstrates that gradual warmup with
their LARS (Layer-wise Adaptive Rate Scaling) technique successfully scaled to
<span class="math notranslate nohighlight">\(kn = 16\text{K}\)</span>.  They used Intel’s KNL processors (i.e. 1600 CPUs) to
attain a top-1 accuracy of 75.3% in 31 minutes <a class="bibtex reference internal" href="#you2017imagenet" id="id5">[YZH+17a]</a>.
However, scaling <span class="math notranslate nohighlight">\(kn = 32\text{K}\)</span> incurred a loss of 0.5% in accuracy.
As an aside, <a class="bibtex reference internal" href="#you2017imagenet" id="id6">[YZH+17a]</a> is a superset of <a class="bibtex reference internal" href="#you2017100" id="id7">[YZH+17b]</a>.</p>
<p>Contrary to the results of <a class="bibtex reference internal" href="#you2017lars" id="id8">[YGG17]</a>, the experiments in
<a class="bibtex reference internal" href="#codreanu2017scale" id="id9">[CPS17]</a> illustrate that LARS is unnecessary when scaling to
<span class="math notranslate nohighlight">\(kn = 32\text{K}\)</span>.  <a class="bibtex reference internal" href="#codreanu2017scale" id="id10">[CPS17]</a> asserts that the
classical 3-step 10-fold decrease is not as good as polynomial decay with the
power of one for the learning rate decay schedule.  Furthermore, the weight
decay (regularization term) matters with a large learning rate.  They set the
weight decay to <span class="math notranslate nohighlight">\(0.00005\)</span> throughout the warmup and for the majority of
the training phase.  During the last 5-7% (4-7 epochs) of training, the learning
rate is decayed with a power of two polynomial, the weight decay doubled to
<span class="math notranslate nohighlight">\(0.0001\)</span>, and data augmentation is disabled.  These techniques enabled
ResNet-50 to be trained using <span class="math notranslate nohighlight">\(kn = 32\text{K}\)</span> without loss of accuracy.
They achieved the highest top-1 accuracy with <span class="math notranslate nohighlight">\(kn = 8\text{K}\)</span> followed by
<span class="math notranslate nohighlight">\(kn = 16\text{K}\)</span>, both of which surpassed the previous state of the art.
What is interesting is that at <span class="math notranslate nohighlight">\(kn = 8\text{K}\)</span>, ResNet-50 attained 73%
accuracy after 5 warmup epochs and 24 training epochs.  For
<span class="math notranslate nohighlight">\(kn \geq 16\text{K}\)</span>, a longer warm-up period (7-10 epochs) is needed and
a constant learning rate is used followed by linear decay.  The accuracy
degrades by 1% beyond <span class="math notranslate nohighlight">\(kn = 32\text{K}\)</span>.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/12/16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour-0"><dl class="citation">
<dt class="bibtex label" id="codreanu2017scale"><span class="brackets">CPS17</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Valeriu Codreanu, Damian Podareanu, and Vikram Saletore. Scale out for large minibatch sgd: residual network training on imagenet-1k with improved accuracy and reduced time to train. <em>arXiv preprint arXiv:1711.04291</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="goyal2017accurate"><span class="brackets">GDollarG+17</span></dt>
<dd><p>Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. <em>arXiv preprint arXiv:1706.02677</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="wang1994optimal"><span class="brackets"><a class="fn-backref" href="#id1">WVJ94</a></span></dt>
<dd><p>Changfeng Wang, Santosh S Venkatesh, and J Stephen Judd. Optimal stopping and effective machine complexity in learning. In <em>Advances in neural information processing systems</em>, 303–310. 1994.</p>
</dd>
<dt class="bibtex label" id="you2017lars"><span class="brackets">YGG17</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. <em>arXiv preprint</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="you2017imagenet"><span class="brackets">YZH+17a</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Yang You, Zhao Zhang, C Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in minutes. <em>CoRR, abs/1709.05011</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="you2017100"><span class="brackets"><a class="fn-backref" href="#id7">YZH+17b</a></span></dt>
<dd><p>Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. 100-epoch imagenet training with alexnet in 24 minutes. <em>arXiv preprint</em>, 2017.</p>
</dd>
</dl>
</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/12/16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>