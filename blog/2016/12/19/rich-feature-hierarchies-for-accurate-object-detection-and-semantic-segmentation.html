<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Watertight Ray/Triangle Intersection" href="../18/watertight-ray-triangle-intersection.html" />
    <link rel="prev" title="Mask R-CNN" href="../20/mask-r-cnn.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.html">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14/layer-normalization.html">Layer Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08/a-fast-learning-algorithm-for-deep-belief-nets.html">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
<li><a class="reference internal" href="#notes">Notes</a><ul>
<li><a class="reference internal" href="#r-cnn">R-CNN</a></li>
<li><a class="reference internal" href="#spp-net">SPP-Net</a></li>
<li><a class="reference internal" href="#fast-r-cnn">Fast R-CNN</a></li>
<li><a class="reference internal" href="#faster-r-cnn">Faster R-CNN</a></li>
<li><a class="reference internal" href="#yolo">YOLO</a></li>
<li><a class="reference internal" href="#ssd">SSD</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../20/mask-r-cnn.html" title="Previous Chapter: Mask R-CNN"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Mask R-CNN</span>
    </a>
  </li>
  <li>
    <a href="../18/watertight-ray-triangle-intersection.html" title="Next Chapter: Watertight Ray/Triangle Intersection"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Watertight Ra... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation">
<h1>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation<a class="headerlink" href="#rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>The object detection accuracy of ensemble systems that employ SIFT and HOG has
plateaued in recent years.  The recent success of CNNs on image classification
call attention to what extent do classification results generalize to object
detection.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p>In an attempt to bridge the gap between image classification and object
detection, the authors propose a system, R-CNN (Regions with CNN features), that
combines region proposals with CNNs.  Their focus is on localizing objects with
a deep neural network and training a high-capacity model with only a small
quantity of annotated detection data.  They solve the CNN localization problem
by operating within the successful <em>recognition using regions</em> paradigm.</p>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>While R-CNN is agnostic to the particular region proposal method, the authors
chose selective search (a.k.a. approximate segmentation at multiple scales) in
order to measure against prior detection work.  They warp each of the 2000
region proposals to the CNN’s input size.  Their inexhaustive evaluations of
different warpings suggest that warping to the tightest square with context
padding resulted in a higher mean average precision (mAP) compared to without
context padding and anisotropic scaling.</p>
<p>The feature extraction step can be performed by any CNN such as AlexNet or
VGGNet.  Experiments on different network architecture reveal that a good
feature extractor gives at least 8% boost in mAP.  Even though the authors opted
for AlexNet to save on computation, R-CNN still tops existing state of the art
methods such as OverFeat by at least 20% in terms of mAP.  OverFeat can be seen
as a special case of R-CNN if one replaces the selective search with a
multi-scale pyramid of regular square regions and changes the per-class
bounding box regressors to a single bounding box regressor.</p>
<p>At test time, they score each extracted feature vector using the SVM trained for
each class.  Given all scored regions in an image, a greedy non-maximum
suppression for each class independently rejects a region if it has an
intersection-over-union (IoU) overlap with a higher scoring selected region
larger than a learned threshold.</p>
<p>Note that CNN fine-tuning uses a different IoU threshold than SVM training.  The
authors conjecture that these hyperparameters need to be different to avoid
overfitting to the limited amount of data.  They did not use the
<span class="math notranslate nohighlight">\((N + 1)\)</span>-way softmax regression classifier as the object detector because
they failed in tuning the system to get higher performance than SVMs.  To fix
mislocalized detections, the authors applied a class-specific bounding box
regression based on CNN pool layer features.  The regression only learns from a
proposal if it is nearby at least one ground-truth box.  This concept of
“nearness” relies on a hardcoded IoU.</p>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>What is the minimum number of input transformations a neural network needs
to be trained on?</p></li>
<li><p>Are the false positives (e.g. localization, similar category, dissimilar
category, background) dominated by network architecture and optimization or
labels?</p></li>
<li><p>Is YOLO beneficial to Faster R-CNN like it was to Fast R-CNN?</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Why not learn the overlap threshold instead of performing a grid search?</p></li>
<li><p>Is treating a particular unit in the network as if it was an object detector
useful for debugging in general?</p></li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Deep learning is one technique that has successfully combined image
classification and object detection.</p>
<p>The authors reported <span class="math notranslate nohighlight">\(\text{fc}_6\)</span> gives the best semantic segmentation
results, but did not provide any numbers on <span class="math notranslate nohighlight">\(\text{fc}_7\)</span>.  This extra
information would help with designing future network architecture.</p>
<p>Their experiments of training SVMs with different CNN layers demonstrate that a
CNN’s representational power mostly comes from its convolutional layers.  The
densely connected layers only yields an additional 10% mAP after fine-tuning.</p>
<p>The distribution of top-ranked false positive types shows that bounding box
regression is necessary to attain the highest possible mAP, but the authors’
approach may not be the most elegant way to achieve this.  Furthermore, it’s
debatable whether the additional complexity of per class linear SVMs is worth
the 4% gain in mAP.</p>
<p>One enlightening insight is that fine-tuning improves robustness (the
highest and lowest normalized mAP) for nearly all characteristics including
occlusion, truncation, viewpoint, and part visibility.  However, it does not
reduce sensitivity (the difference between max and min).</p>
<p>In an attempt to shorten the feature extraction step of R-CNN,
<a class="bibtex reference internal" href="#he2014spatial" id="id1">[HZRS14]</a> proposes a spatial pyramid pooling (SPP) layer that
operates directly on the feature map of each training example.  The experiments
demonstrate comparable accuracy to R-CNN but with two orders of magnitude
speedup.</p>
<p>The authors of the follow up work Fast R-CNN realized that the multi-stage
training is complicated and possibly not necessary, so they propose to train
the detector in a single stage, end-to-end by jointly optimizing a softmax
classifier and bounding-box regressors <a class="bibtex reference internal" href="#girshick2015fast" id="id2">[Gir15]</a>.  Their
empirical results indicate:</p>
<ul class="simple">
<li><p>Multi-task training improves pure classification accuracy relative to training
for classification alone.</p></li>
<li><p>Multi-scale training approach offers only a small increase in mAP at a large
cost in compute time because deep CNNs are adept at directly learning scale
invariance.</p></li>
<li><p>The simple softmax classifier slightly outperforms SVMs in mAP.</p></li>
<li><p>Augmenting the training data by increasing the amount of object proposals
(e.g. 1k to 10k) actually hurts accuracy.</p></li>
<li><p>Fine-tuning the entire network plays an important role in deep networks
compared to shallow networks like SPP-net.</p>
<ul>
<li><p>The first three convolutional layers do not have to be fine-tuned because
their impact to mAP is less than 0.3 points.</p></li>
</ul>
</li>
</ul>
<p>Even though Fast R-CNN reduced the detection pipeline’s processing time, the
system still takes seconds to operate per image due to region proposals.  One
solution is to replace the selective search method with a region proposal
network (RPN) <a class="bibtex reference internal" href="#ren2015faster" id="id3">[RHGS15]</a>.  This pipeline enabled Faster R-CNN to
operate at interactive rates with even higher mAP.  The network’s loss function
is the sum of the RPN loss and Fast R-CNN loss.  The empirical results show that
this approximate joint training scheme is faster and matches the accuracy of
alternating training.  Note that the latter is not based on any fundamental
principles while the former ignores the undefined
<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial RoI[x]}\)</span> where <span class="math notranslate nohighlight">\(x\)</span> denotes the proposal
boxes’ coordinates that are also network responses.  Exact joint training
requires an RoI pooling layer that is differentiable w.r.t. the box coordinates,
possibly via RoI warping.  One interesting result is that the multi-scale anchor
boxes alone are not sufficient for accurate detection, and aspect ratios have
insignificant effects on detection accuracy.</p>
<p>To further reduce the system latency, <a class="bibtex reference internal" href="#redmon2016you" id="id4">[RDGF16]</a> models detection as
a regression problem and uses features from the entire image to predict all
bounding boxes simultaneously.  Their system (YOLO) imposes strong spatial
constraints on bounding box predictions since each grid cell only predicts two
boxes and can only have one class.  This spatial constraint and coarse features
causes YOLO to struggle with small objects that appear in groups and fail to
generalize to objects in new or unusual aspect ratios or configurations.  Their
unified detection takes approximately 22ms, but gives 10 mAP less than Faster
R-CNN due to incorrect localizations.  Even though combining YOLO with Fast
R-CNN gives impressive accuracy, it is not better than Faster R-CNN.</p>
<p>Notice that Faster R-CNN is fully convolutional but requires a second stage to
classify the bounding boxes, whereas YOLO directly predicts the multi-class
probability vector and offsets for <span class="math notranslate nohighlight">\(K\)</span> boxes using a fully-connected
layers.  One way to combine the best of both solutions without resampling
pixels or feature maps is given in <a class="bibtex reference internal" href="#liu2016ssd" id="id5">[LAE+16]</a>.  Their system (SSD) has
less localization error because it directly learns to regress the object shape
and classify object categories in a single step.  However, since the parameters
are shared for multiple categories, the error due to similar object categories
is higher.  Furthermore, the system is very sensitive to the bounding box size
and performs worse on smaller objects than bigger objects.  Nevertheless, the
system latency is only twice that of YOLO with much higher accuracy and
robustness than Faster R-CNN due to multi-scale feature maps and default boxes.
Note that the higher accuracy requires the use of data augmentation e.g.
horizontal flip, random crop, color distortion, and random expansion.</p>
</div>
<div class="section" id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="r-cnn">
<h3>R-CNN<a class="headerlink" href="#r-cnn" title="Permalink to this headline">¶</a></h3>
<p>The training pipeline consists of</p>
<ol class="arabic simple">
<li><p>Offline pre-training of a CNN <span class="math notranslate nohighlight">\(M\)</span> for image classification.</p></li>
<li><p>Fine-tune <span class="math notranslate nohighlight">\(M\)</span> for object detection by replacing the softmax classifier
with one that has the desired number of classes plus one background class.</p></li>
<li><p>Freeze <span class="math notranslate nohighlight">\(M\)</span>’s weights and cache the training data’s feature vectors.</p>
<ul class="simple">
<li><p>Training data includes all the region proposals.</p></li>
<li><p>For AlexNet, the feature vectors could be <span class="math notranslate nohighlight">\(\text{pool}_5\)</span>,
<span class="math notranslate nohighlight">\(\text{fc}_6\)</span>, or <span class="math notranslate nohighlight">\(\text{fc}_7\)</span>.</p></li>
</ul>
</li>
<li><p>Independently train a linear SVM for each category on the feature vectors
using the hard negative mining method.</p></li>
<li><p>Independently train a linear bounding box regressor for each category to
refine the proposal’s initial region of interest.</p>
<ul class="simple">
<li><p>For AlexNet, the regression can be modeled as a linear function of the
<span class="math notranslate nohighlight">\(\text{pool}_5\)</span> features.</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="spp-net">
<h3>SPP-Net<a class="headerlink" href="#spp-net" title="Permalink to this headline">¶</a></h3>
<p>The region proposals are still being generated using selective search.  However,
instead of passing each region proposal through the CNN, SPP-net generates a
feature map of the entire input image using the CNN <a class="bibtex reference internal" href="#he2014spatial" id="id6">[HZRS14]</a>.  In
order to map the window of each proposed region to the feature map, the authors
project the corner point of a window onto a pixel in the feature map such that
this corner point in the image domain is closest to the center of the receptive
field of that feature map pixel.</p>
<p>The spatial pyramid pooling layer does not have any weights.  It replaces the
last pooling layer in R-CNN, and outputs a <span class="math notranslate nohighlight">\(kM\)</span>-dimensional vector
regardless of image size and scale.  Here <span class="math notranslate nohighlight">\(k\)</span> is the number of filters in
the last convolutional layer, and <span class="math notranslate nohighlight">\(M\)</span> is the number of bins.  The vector
is akin to an image pyramid that uses bins instead of pixels.  In each spatial
bin, the authors (max) pool the responses of each filter.</p>
<p>By decoupling the convolutional layers from the fully-connected layers, SPP-net
enables the former to accept inputs of arbitrary sizes while satisfying the
latter’s requirement of fixed-size input.  Consequently, the fine-tuning phase
now only needs to modify the fully-connected layers.</p>
</div>
<div class="section" id="fast-r-cnn">
<h3>Fast R-CNN<a class="headerlink" href="#fast-r-cnn" title="Permalink to this headline">¶</a></h3>
<p>While R-CNN supports training all layers of the network, SPP-net cannot because
of its SPP layer optimization.  <a class="bibtex reference internal" href="#girshick2015fast" id="id7">[Gir15]</a> instead propose a
Region of Interest (RoI) pooling layer, which is a special case of the SPP layer
with one pyramid level.  Each proposed RoI may have a very large receptive
field, so mini-batches are sampled hierarchically: first sample <span class="math notranslate nohighlight">\(N\)</span> images
and then sample <span class="math notranslate nohighlight">\(R / N\)</span> RoIs from each image.  This enables RoIs from the
same image to share computation and memory in the forward and backward passes.
Even though the RoIs from the same image are correlated, the authors achieved
good results with <span class="math notranslate nohighlight">\((N = 2, R = 128)\)</span> and did not notice any slowdown of
training convergence.</p>
<p>In order to fine-tune the entire network via SGD, the authors define two loss
branches after the fully-connected layers.  The sum of the softmax classifier
and per-class linear bounding-box regressors is the total loss <span class="math notranslate nohighlight">\(L\)</span>.  For
background RoIs, there is no notion of a ground-truth bounding box.  The errors
are backpropagated through the RoI pooling layer, which is just like max pooling
except that pooling regions overlap:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\Pr}{\mathop{\mathrm{Pr}}}
\frac{\partial L}{\partial x_i}
 &amp;= \sum_r \sum_j
      \frac{\partial y_{rj}}{\partial x_i}
      \frac{\partial L}{\partial y_{rj}}\\
 &amp;= \sum_r \sum_j
      \left[ i = i^*_{rj} \right] \frac{\partial L}{\partial y_{rj}}
\qquad \text{where} \qquad
i^*_{rj} = \argmax_{i' \in \mathcal{R}(r, j)} x_{i'}\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\mathcal{R}(r, j)\)</span> is the index set of inputs in the sub-window over
the RoI <span class="math notranslate nohighlight">\(r\)</span> which the output unit <span class="math notranslate nohighlight">\(y_{rj}\)</span> max pools.  Here
<span class="math notranslate nohighlight">\(x_i\)</span> is the <span class="math notranslate nohighlight">\(i^\text{th}\)</span> activation input into the RoI pooling
layer, and it may be assigned to several different outputs <span class="math notranslate nohighlight">\(y_{rj}\)</span>.</p>
<p>At test time for each RoI <span class="math notranslate nohighlight">\(r\)</span>, Fast R-CNN’s forward pass outputs a class
posterior probability distribution <span class="math notranslate nohighlight">\(p\)</span> and a set of predicted bounding-box
offsets relative to <span class="math notranslate nohighlight">\(r\)</span>.  Each class independently runs non-maximum
suppression as in R-CNN but uses the detection confidence
<span class="math notranslate nohighlight">\(\mathop{\mathrm{Pr}}\left( \text{class} = k \mid r \right) = p_k\)</span> as its
criterion.  One can also trade a small drop in mAP to further reduce detection
time via approximating each fully-connected layer’s weight matrix by a matrix of
lower rank.</p>
</div>
<div class="section" id="faster-r-cnn">
<h3>Faster R-CNN<a class="headerlink" href="#faster-r-cnn" title="Permalink to this headline">¶</a></h3>
<p>A RPN takes as input an image (e.g. feature map) of arbitrary size, slides a
<span class="math notranslate nohighlight">\(n \times n\)</span> spatial window over each pixel of the input, and outputs a
set of rectangular object proposals.  At each sliding window location, the RPN
classifies what is under the window as object or not object.  If it is an
object, the RPN estimates some bounding box coordinates.  If it is not, the
estimated coordinates are nonsensical and should be ignored.</p>
<p>The maximum number of possible proposals for each location is limited to
<span class="math notranslate nohighlight">\(k\)</span> i.e. <span class="math notranslate nohighlight">\(2k + 4k\)</span> outputs per pixel.  The <span class="math notranslate nohighlight">\(k\)</span> proposals
are parameterized relative to <span class="math notranslate nohighlight">\(k\)</span> anchor boxes.  Each anchor box has a
constant scale and aspect ratio, both of which are hand-picked.  This pyramid of
anchors is centered at the sliding window in question, and the RPN learns the
appropriate shared weights to estimate <span class="math notranslate nohighlight">\(2k + 4k\)</span> outputs at each position.
The <span class="math notranslate nohighlight">\(2k\)</span> represents the two-class softmax layer, and the <span class="math notranslate nohighlight">\(4k\)</span>
embodies R-CNN’s bounding box regression.</p>
<p>Given the feature map that is the output of ReLU on the last convolutional
layer, <a class="bibtex reference internal" href="#ren2015faster" id="id8">[RHGS15]</a> implements the RPN as two sibling
<span class="math notranslate nohighlight">\(1 \times 1\)</span> convolutional layers whose depth is <span class="math notranslate nohighlight">\(2k\)</span> and <span class="math notranslate nohighlight">\(4k\)</span>
respectively.  This approach guarantees translation invariance.  Furthermore,
the pyramid only relies on inputs of a single scale and uses filters of a single
size.  In order to support the multi-scale design, a set of <span class="math notranslate nohighlight">\(k\)</span> bounding
box regressors are learned for each combination of scale and aspect ratio.
Unlike previous RoI-based methods, these regressors do not share weights.</p>
</div>
<div class="section" id="yolo">
<h3>YOLO<a class="headerlink" href="#yolo" title="Permalink to this headline">¶</a></h3>
<p>The system divides the input image into a <span class="math notranslate nohighlight">\(S \times S\)</span> grid.  Each grid
cell predicts <span class="math notranslate nohighlight">\(B\)</span> bounding boxes and confidence scores for those boxes.
Each bounding box’s center <span class="math notranslate nohighlight">\((x, y)\)</span> is relative to the bounds of the grid
cell whereas its <span class="math notranslate nohighlight">\((w, h)\)</span> are relative to the whole image.  The confidence
score <span class="math notranslate nohighlight">\(\Pr(\text{Object}) \text{IoU}\)</span> captures whether the box contains an
object and the overlap between the predicted box and any ground truth box.</p>
<p>YOLO only predicts <span class="math notranslate nohighlight">\(C\)</span> conditional class probabilities
<span class="math notranslate nohighlight">\(\Pr\left( \text{Class}_i \mid \text{Object} \right)\)</span> per grid cell
regardless of the number of boxes <span class="math notranslate nohighlight">\(B\)</span>.  The class-specific confidence
scores is attained through marginalization:</p>
<div class="math notranslate nohighlight">
\[\Pr\left( \text{Class}_i \mid \text{Object} \right)
    \Pr(\text{Object}) \text{IoU} =
\Pr(\text{Class}_i) \text{IoU}.\]</div>
<p>The architecture is a variation of GoogLeNet with <span class="math notranslate nohighlight">\(24\)</span> convolutional
layers followed by two fully-connected layers with the last layer being a
<span class="math notranslate nohighlight">\(S \times S \times (5B + C)\)</span> tensor.  Besides pretraining the first
<span class="math notranslate nohighlight">\(20\)</span> convolutional layers on ImageNet and applying non-maximum suppression
on the bounding boxes per grid cell, <a class="bibtex reference internal" href="#redmon2016you" id="id9">[RDGF16]</a> also had to finagle
the optimization procedure.  They increase the loss from the bounding box
coordinate predictions and decrease the loss from confidence predictions for
boxes that do not contain objects because many grid cells do not contain any
object.  Their system predicts the square root of the bounding box width and
height so that small deviations in large boxes matter less than in small
boxes.  However, their loss function treats errors in small bounding boxes
versus large bounding boxes the same.  Achieving high accuracy requires the
learning rate schedule to imitate the typical <span class="xref std std-doc">gradual warmup strategy</span>.</p>
</div>
<div class="section" id="ssd">
<h3>SSD<a class="headerlink" href="#ssd" title="Permalink to this headline">¶</a></h3>
<p>Given the convolutional layers of VGG-16 through Conv5_3 layer as a base, the
authors add convolutional feature layers to the end.  Although these layers
decrease in size progressively, their output can be defined by a kernel
filter of size <span class="math notranslate nohighlight">\(3 \times 3 \times (C + 4) K_i\)</span>.  Here <span class="math notranslate nohighlight">\(C\)</span> is the
number of classes, four denotes the refinements to some fixed default box,
and <span class="math notranslate nohighlight">\(K_i\)</span> is the number of default boxes for feature map <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>A default box is essentially a multi-scale anchor box <a class="bibtex reference internal" href="#liu2016ssd" id="id10">[LAE+16]</a>.  The
predicted bounding box offsets are relative to a default box’s extents relative
to each feature map (pixel) location.  The coordinates are normalized w.r.t.
image dimensions to achieve invariance to absolute image size i.e. think UV
texture space.  The default boxes do not have to correspond to the actual
receptive fields of each layer.</p>
<p>Suppose the network uses <span class="math notranslate nohighlight">\(m\)</span> feature maps for prediction.  The scale of
the default boxes for each feature map is given by</p>
<div class="math notranslate nohighlight">
\[s_k = s_\min + \frac{s_\max - s_\min}{m - 1} (k - 1)\]</div>
<p>where the lowest layer and highest layer has a scale of <span class="math notranslate nohighlight">\(s_\min\)</span> and
<span class="math notranslate nohighlight">\(s_\max\)</span> respectively.  The authors also impose different aspect ratios
for the default boxes.</p>
<p>For a feature map <span class="math notranslate nohighlight">\(i\)</span> of size <span class="math notranslate nohighlight">\(m \times n\)</span>, the network generates
<span class="math notranslate nohighlight">\((C + 4) K_i mn\)</span> outputs.  To handle the large number of default boxes,
the authors match each ground truth (GT) box to closest default box.  They also
match each GT box to all unassigned default boxes with a high IoU.  To deal with
the unbalanced number of true positives (TP) vs false positives (FP), they use
hard negative mining on the worst misclassified FPs i.e. keep TP:FP ratio fixed
at 1:3.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/12/19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation-0"><dl class="citation">
<dt class="bibtex label" id="girshick2015fast"><span class="brackets">Gir15</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Ross Girshick. Fast r-cnn. <em>arXiv preprint arXiv:1504.08083</em>, 2015.</p>
</dd>
<dt class="bibtex label" id="girshick2014rich"><span class="brackets">GDDM14</span></dt>
<dd><p>Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 580–587. 2014.</p>
</dd>
<dt class="bibtex label" id="he2014spatial"><span class="brackets">HZRS14</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In <em>european conference on computer vision</em>, 346–361. Springer, 2014.</p>
</dd>
<dt class="bibtex label" id="liu2016ssd"><span class="brackets">LAE+16</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: single shot multibox detector. In <em>European conference on computer vision</em>, 21–37. Springer, 2016.</p>
</dd>
<dt class="bibtex label" id="redmon2016you"><span class="brackets">RDGF16</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: unified, real-time object detection. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 779–788. 2016.</p>
</dd>
<dt class="bibtex label" id="ren2015faster"><span class="brackets">RHGS15</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In <em>Advances in neural information processing systems</em>, 91–99. 2015.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/12/19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>