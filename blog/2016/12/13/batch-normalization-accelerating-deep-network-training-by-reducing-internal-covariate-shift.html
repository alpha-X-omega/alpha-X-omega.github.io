<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Deep Residual Learning for Image Recognition" href="../12/deep-residual-learning-for-image-recognition.html" />
    <link rel="prev" title="Layer Normalization" href="../14/layer-normalization.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.html">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.html">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14/layer-normalization.html">Layer Normalization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08/a-fast-learning-algorithm-for-deep-belief-nets.html">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/23/training-products-of-experts-by-minimizing-contrastive-divergence.html">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/19/a-tutorial-on-helmholtz-machines.html">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11/01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/27/sphinx-on-github-pages.html">Sphinx on GitHub Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
<li><a class="reference internal" href="#notes">Notes</a><ul>
<li><a class="reference internal" href="#batch-normalizing-transform">Batch Normalizing Transform</a><ul>
<li><a class="reference internal" href="#batch-normalized-convolutional-networks">Batch-Normalized Convolutional Networks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#batch-renormalization">Batch Renormalization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../14/layer-normalization.html" title="Previous Chapter: Layer Normalization"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Layer Normalization</span>
    </a>
  </li>
  <li>
    <a href="../12/deep-residual-learning-for-image-recognition.html" title="Next Chapter: Deep Residual Learning for Image Recognition"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Deep Residual... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">
<h1>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<a class="headerlink" href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>Stochastic gradient descent is an effective way of training deep networks,
assuming the learning rate schedule and the initial values for the model
parameters are appropriately tuned.  Hyperparameter tuning is further
exacerbated by the change in the distributions of layers’ inputs, even though
the training and test data come from the same distribution.  The change in the
distribution of network activations due to the change in network parameters
during training is called internal covariate shift.</p>
<p>To combat internal covariate shift, one solution is to linearly transform the
inputs to each layer such that the inputs are decorrelated and have zero means
with unit variances.  Consider a layer <span class="math notranslate nohighlight">\(x = u + b\)</span> with input <span class="math notranslate nohighlight">\(u\)</span>
and bias <span class="math notranslate nohighlight">\(b\)</span>.  Whitening the inputs yields
<span class="math notranslate nohighlight">\(\newcommand{\E}[2][]{\mathop{\mathrm{E}_{#1}}\left[ #2 \right]}
\hat{x} = x - \E{x}\)</span> where
<span class="math notranslate nohighlight">\(\mathcal{X} = \left\{ x_1, \ldots, x_N \right\}\)</span> and
<span class="math notranslate nohighlight">\(\E{x} = \frac{1}{N} \sum_{i = 1}^N x_i\)</span>.  The corresponding gradient
descent update is <span class="math notranslate nohighlight">\(\Delta b = -\frac{\partial l}{\partial b}\)</span>, and the
next iteration will be</p>
<div class="math notranslate nohighlight">
\[u + (b + \Delta b) - \E{u + \left( b + \Delta b \right)} =
x - \E{x} + \Delta b - \E{\Delta b}.\]</div>
<p>Suppose the gradient descent step defines</p>
<div class="math notranslate nohighlight">
\[\frac{\partial l}{\partial b} =
\frac{\partial l}{\partial x} \frac{\partial x}{\partial b} =
\frac{\partial l}{\partial x}
\qquad \text{instead of} \qquad
\frac{\partial l}{\partial b} =
\frac{\partial l}{\partial \hat{x}} \frac{\partial \hat{x}}{\partial b} =
\frac{\partial l}{\partial \hat{x}}
  \left( 1 - \frac{\partial \E{x}}{\partial b} \right).\]</div>
<p>The combination of the update to <span class="math notranslate nohighlight">\(b\)</span> and subsequent change in
normalization leads to no change in the output of the layer because</p>
<div class="math notranslate nohighlight">
\[\E{\Delta b} =
\E{-\frac{\partial l}{\partial x}} =
\Delta b.\]</div>
<p>As the training continues, <span class="math notranslate nohighlight">\(b\)</span> grows indefinitely while the loss remains
fixed.  To address this issue when <span class="math notranslate nohighlight">\(\hat{x} = f(\mathbf{x}, \mathcal{X})\)</span>
is an arbitrary transformation, backpropagation needs to account for the
Jacobian <span class="math notranslate nohighlight">\(\frac{\partial \hat{x}}{\partial \mathcal{X}}\)</span> after
every parameter update.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p>Instead of whitening the features over the inputs and outputs jointly, the
authors propose batch normalization (BN): independently transform each scalar
feature to have zero mean and unit variance.  Rather than using the entire
training set <span class="math notranslate nohighlight">\(\mathcal{X} = \left\{ \mathbf{x}_1, \ldots, \mathbf{x}_N \right\}\)</span>
to produce the desired statistics, each mini-batch
<span class="math notranslate nohighlight">\(\mathcal{B}_j = \left\{ \mathbf{x}_{j + 1}, \ldots, \mathbf{x}_{j + m} \right\}\)</span>
generates estimates of the sample mean and <a class="reference external" href="https://en.wikipedia.org/wiki/Variance#Sample_variance">biased sample variance</a> of each
activation.</p>
<p>For a layer with input
<span class="math notranslate nohighlight">\(\mathbf{x} = \left( x^{(1)}, \ldots, x^{(d)} \right)\)</span>, normalize each
dimension</p>
<div class="math notranslate nohighlight">
\[\newcommand{\Var}[2][]{\mathop{\mathrm{Var}_{#1}}\left[ #2 \right]}
\hat{x}^{(k)} =
\frac{
  x^{(k)} - \E[\mathcal{B}_j]{x^{(k)}}
}{
  \sqrt{\Var[\mathcal{B}_j]{x^{(k)}} + \epsilon}
}.\]</div>
<p>To ensure the transformation can represent the identity mapping, the authors
introduce a pair of learned parameters for each activation <span class="math notranslate nohighlight">\(x^{(k)}\)</span> to
scale and shift the normalized value such that</p>
<div class="math notranslate nohighlight">
\[y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}.\]</div>
<p>BN makes backpropagation through a layer invariant to the scale of its
parameters.  Such normalization have been demonstrated to speed up convergence
<a class="bibtex reference internal" href="../../11/09/efficient-backprop.html#lecun-98x" id="id1">[LeCunBOMuller98]</a>:</p>
<ul class="simple">
<li><p>Nonzero mean in the input variables creates a very large eigenvalue (i.e.
large condition number), which translates to inputs having a large variation
in spread along different directions of the input space.</p></li>
<li><p>If the inputs are correlated, enforcing unit variance will not make the error
surface spherical, but it will reduce its eccentricity.</p></li>
</ul>
<p>To make the inference step deterministic, the normalization step for a
particular activation</p>
<div class="math notranslate nohighlight">
\[\hat{x}^{(k)} =
\frac{
  x^{(k)} - \E[\mathcal{X}]{x^{(k)}}
}{
  \sqrt{\Var[\mathcal{X}]{x^{(k)}} + \epsilon}
}\]</div>
<p>uses the entire training population statistics.  Since the true population mean
is unknown, it is estimated as the sample mean</p>
<div class="math notranslate nohighlight">
\[\begin{split}\E[\mathcal{X}]{x^{(k)}}
 &amp;\gets \frac{1}{N} \sum_i x_i^{(k)}\\
 &amp;= \frac{1}{N / m} \sum_j
      \frac{1}{m} \sum_{\mathbf{x}_i \in \mathcal{B}_j} x_i^{(k)}\\
 &amp;= \frac{1}{N / m} \sum_j \mu_{\mathcal{B}_j}.\end{split}\]</div>
<p>Given the sample mean, the unbiased sample variance can be obtained using
<a class="reference external" href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a> factor <span class="math notranslate nohighlight">\(\frac{m}{m - 1}\)</span>.  Assuming the underlying
variance of each mini-batch is the same, it is estimated as the
<a class="reference external" href="https://en.wikipedia.org/wiki/Pooled_variance">pooled variance</a></p>
<div class="math notranslate nohighlight">
\[\begin{split}\Var[\mathcal{X}]{x^{(k)}}
 &amp;\gets \frac{
          \sum_j
            \left( \left\vert \mathcal{B}_j \right\vert - 1 \right)
            \frac{m}{m - 1} \Var[\mathcal{B}_j]{x^{(k)}}
        }{
          \sum_j
            \left( \left\vert \mathcal{B}_j \right\vert - 1 \right)
        }\\
 &amp;= \frac{1}{N / m} \sum_j \frac{m}{m - 1} \sigma_{\mathcal{B}_j}^2.\end{split}\]</div>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>The authors studied the evolution of input distributions over the course of
training on the MNIST dataset.  BN helps the network train faster and achieve
higher accuracy by making the distribution more stable and reducing the internal
covariate shift.</p>
<p>For a large dataset like ImageNet, adding BN on top of Inception yielded modest
benefits.  In order to reduce the number of epochs by an order of magnitude
while increasing accuracy by three percent, the network and its
hyperparameters need to be modified as follows:</p>
<ul class="simple">
<li><p>Increase the learning rates.</p></li>
<li><p>Remove dropout.</p></li>
<li><p>Reduce <span class="math notranslate nohighlight">\(L_2\)</span> weight regularization.</p></li>
<li><p>Accelerate the learning rate decay.</p></li>
<li><p>Remove Local Response Normalization.</p></li>
<li><p>Shuffle training examples more thoroughly to prevent the same examples from
always appearing in a mini-batch together.</p></li>
</ul>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Does Bessel’s correction matter, or will the learned weights account for it?</p></li>
<li><p>What happens to a network’s convergence when larger weights lead to smaller
gradients?</p></li>
<li><p>The authors conjecture that BN may lead the layer Jacobians to have singular
values close to one.  How?</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>To what extent did dropout improve the accuracy of Modified BN-Inception in
single-network classification?</p></li>
<li><p>Since a Gaussian variable has the largest entropy among all random variables
of equal variance, how did the kurtosis and negentropy of each dimension
compare in terms of nongaussianity?</p></li>
<li><p>I am not convinced of interpreting batch renormalization as a null-space
projection.  Since <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(d\)</span> gradually increases over many
training steps, batch renormalization is essentially scaling the gradients.
A more reasonable hypothesis is that this hack tries to imitate how neurons
amplify the signal it receives.</p></li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>BN is an effective technique to reduce internal covariate shift and achieve a
stable distribution of activation values throughout training.  Its effectiveness
diminishes when the training mini-batches are small, or do not consist of
independent samples <a class="bibtex reference internal" href="#ioffe2017batch" id="id2">[Iof17]</a>.  For small mini-batches, the
estimates of the mean and variance become less accurate.  Furthermore, it is
common to bias the mini-batch sampling to include sets of examples that are
known to be related.  Alas, the model learns to predict labels for images that
come in a set, where each image has a counterpart with the same label.  This
does not directly translate to classifying images individually.</p>
<p>During inference, the upper layers (whose inputs are normalized using the
mini-batch) are trained on representations different from those computed in
inference (whose inputs are normalized using the population statistics).  A
naive solution is to use the moving averages <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> to
perform the normalization during training.  This causes the model parameters to
blow up because the gradient optimization and the normalization counteracts each
other.  To handle the preceding issues, <a class="bibtex reference internal" href="#ioffe2017batch" id="id3">[Iof17]</a> proposes batch
renormalization as an extension to BN that allows a model with a batch size less
than 32 to train faster and achieve a higher accuracy.  However, the
dependencies between training cases make batch renormalization inappropriate for
recurrent neural networks and online learning tasks.  Moreover, there is
uncertainty as to whether a batch size of 32 is problematic in the future.</p>
<p><a class="bibtex reference internal" href="#ioffe2015batch" id="id4">[IS15]</a> asserts that BN should be applied before the nonlinearity
since that is where matching the first and second moments is more likely to
result in a stable distribution.  Yet, the authors’ actual implementation oddly
<a class="reference external" href="https://github.com/keras-team/keras/issues/1802#issuecomment-187966878">applies BN after ReLU</a>.  In addition, the authors claimed dropout can be
removed or weighted less, but only presented evidence of the latter.  They
should have presented the accuracy of their top ensemble network without
dropout because their differential over the previous state of the art is barely
1%.</p>
<p>An alternative technique one should be wary of is weight normalization (WN).  WN
decouples the direction of the weights from their norm by reparameterizing the
optimization problem <a class="bibtex reference internal" href="#salimans2016weight" id="id5">[SK16]</a>.  Even though WN achieves better
and faster training accuracy, the final test accuracy is significantly lower
than BN <a class="bibtex reference internal" href="#gitman2017comparison" id="id6">[GG17]</a>.  The CIFAR-10 results presented in
<a class="bibtex reference internal" href="#salimans2016weight" id="id7">[SK16]</a> is practically insignificant because its seventeen
layer network does not need normalization to attain a final accuracy comparable
to that of a normalized network.  A more serious issue is WN’s assumption that
its weight matrices are approximately orthogonal.  This is invalid because each
gradient update increases correlations between different neurons.  In
consequence, WN is prone to overfitting even with dropout and weight decay.</p>
</div>
<div class="section" id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batch-normalizing-transform">
<h3>Batch Normalizing Transform<a class="headerlink" href="#batch-normalizing-transform" title="Permalink to this headline">¶</a></h3>
<p>Given a mini-batch
<span class="math notranslate nohighlight">\(\mathcal{B} = \left\{ x_1, \ldots, x_m \right\}\)</span> for a particular
activation, the normalizing transform consists of</p>
<div class="math notranslate nohighlight">
\[\begin{split}\DeclareMathOperator{\BN}{\mathrm{BN}}
y_i &amp;\gets \gamma \hat{x}_i + \beta \equiv \BN_{\gamma, \beta}(x_i)\\
\hat{x}_i &amp;\gets \frac{x_i - \mu_{\mathcal{B}}}{\sigma_{\mathcal{B}}}\\
\mu_{\mathcal{B}} &amp;\gets \frac{1}{m} \sum_{i = 1}^m x_i\\
\sigma_{\mathcal{B}}
 &amp;\gets \sqrt{\epsilon + m^{-1} \sum_{i = 1}^m (x_i - \mu_{\mathcal{B}})^2}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a constant.  The gradients with respect to the
parameters of the BN transform are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial \gamma} &amp;=
\sum_{i = 1}^m \frac{\partial l}{\partial y_i}
               \frac{\partial y_i}{\partial \gamma} =
\sum_i \frac{\partial l}{\partial y_i} \hat{x}_i
\\\\
\frac{\partial l}{\partial \beta} &amp;=
\sum_{i = 1}^m \frac{\partial l}{\partial y_i}
               \frac{\partial y_i}{\partial \beta} =
\sum_i \frac{\partial l}{\partial y_i}
\\\\
\frac{\partial l}{\partial \sigma_{\mathcal{B}}} &amp;=
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               \frac{\partial \hat{x}_i}{\partial \sigma_{\mathcal{B}}} =
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               (x_i - \mu_{\mathcal{B}}) \frac{-1}{\sigma_{\mathcal{B}}^2}
\\\\
\frac{\partial l}{\partial \hat{x}_i} &amp;=
\frac{\partial l}{\partial y_i} \frac{\partial y_i}{\partial \hat{x}_i} =
\frac{\partial l}{\partial y_i} \gamma
\\\\
\frac{\partial l}{\partial \mu_{\mathcal{B}}} &amp;=
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               \frac{\partial \hat{x}_i}{\partial \mu_{\mathcal{B}}} =
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               \frac{-1}{\sigma_{\mathcal{B}}}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial \hat{x}_i}{\partial \mu_{\mathcal{B}}}
 &amp;= \frac{-1}{\sigma_{\mathcal{B}}} +
    \left( x_i - \mu_{\mathcal{B}} \right)
      \frac{-1}{\sigma_{\mathcal{B}}^2} \frac{1}{2 \sigma_{\mathcal{B}}}
      \frac{2}{m} \sum_{j = 1}^m (x_j - \mu_{\mathcal{B}}) (-1)\\
 &amp;= \frac{-1}{\sigma_{\mathcal{B}}} +
    \frac{\partial \hat{x}_i}{\partial \sigma_{\mathcal{B}}}
      \frac{1}{\sigma_{\mathcal{B}}}
      \left( \mu_{\mathcal{B}} - \frac{1}{m} \sum_{j = 1}^m x_j \right)\\
 &amp;= \frac{-1}{\sigma_{\mathcal{B}}}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial x_i}
 &amp;= \sum_{j = 1}^m
      \frac{\partial l}{\partial y_j}
      \frac{\partial y_j}{\partial \hat{x}_j}
      \frac{\partial \hat{x}_j}{\partial x_i}\\
 &amp;= \sum_{j = 1}^m
      \frac{\partial l}{\partial \hat{x}_j}
      \left[
        \left( \mathbb{I}_i(j) - \frac{1}{m} \right)
          \sigma_{\mathcal{B}}^{-1} +
        (x_j - \mu_{\mathcal{B}})
          \frac{-1}{\sigma_{\mathcal{B}}^2} \frac{1}{2 \sigma_{\mathcal{B}}}
          \frac{2}{m} (x_i - \mu_{\mathcal{B}})
      \right]\\
 &amp;= \frac{\partial l}{\partial \hat{x}_i} \frac{1}{\sigma_{\mathcal{B}}} +
    \frac{\partial l}{\partial \mu_{\mathcal{B}}} \frac{1}{m} +
    \frac{\partial l}{\partial \sigma_{\mathcal{B}}}
      \frac{x_i - \mu_{\mathcal{B}}}{m \sigma_{\mathcal{B}}}\end{split}\]</div>
<div class="section" id="batch-normalized-convolutional-networks">
<h4>Batch-Normalized Convolutional Networks<a class="headerlink" href="#batch-normalized-convolutional-networks" title="Permalink to this headline">¶</a></h4>
<p>Recall that <a class="reference internal" href="../12/deep-residual-learning-for-image-recognition.html"><span class="doc">fully-connected and convolutional layers can be formulated as an affine transformation</span></a>
followed by an element-wise nonlinearity:</p>
<div class="math notranslate nohighlight">
\[\mathbf{z} =
\mathop{g}(\mathbf{x}) =
\mathop{g}(\mathbf{W} \mathbf{u} + \mathbf{b})\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> are learned parameters of the
model, and <span class="math notranslate nohighlight">\(\mathop{g}\)</span> is the nonlinearity.  The BN transform is
applied as</p>
<div class="math notranslate nohighlight">
\[\BN(\mathbf{x}) =
\BN(\mathbf{W} \mathbf{u} + \mathbf{b}) =
\BN(\mathbf{W} \mathbf{u})\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> is subsumed by <span class="math notranslate nohighlight">\(\beta\)</span>.  The authors reason that
BN should be applied to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> instead of <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> because
the former is more Gaussian than the latter.  This phenomenon follows from the
Central Limit Theorem, which states that the distribution of a sum of
independent random variables tends toward a Gaussian distribution
<a class="bibtex reference internal" href="#mlugos134f2011" id="id8">[Lug]</a><a class="bibtex reference internal" href="#hyvarinen2000independent" id="id9">[HyvarinenO00]</a>.</p>
<p>For convolutional layers, <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the set of all values in a
feature map across both the elements of a mini-batch and spatial locations.  For
a mini-batch of size <span class="math notranslate nohighlight">\(m\)</span> and feature maps of size <span class="math notranslate nohighlight">\(p \times q\)</span>, the
effective mini-batch size is <span class="math notranslate nohighlight">\(m′ = | \mathcal{B} | = mpq\)</span>.  The pair of
parameters <span class="math notranslate nohighlight">\(\gamma^{(k)}\)</span> and <span class="math notranslate nohighlight">\(\beta^{(k)}\)</span> is now per feature map
rather than per activation.</p>
</div>
</div>
<div class="section" id="batch-renormalization">
<h3>Batch Renormalization<a class="headerlink" href="#batch-renormalization" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mu\)</span> be an estimate of the mean of <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(\sigma\)</span> be
an estimate of its standard deviation, computed as</p>
<div class="math notranslate nohighlight">
\[\mu \gets \mu + \alpha (\mu_{\mathcal{B}} - \mu)
\qquad \text{and} \qquad
\sigma \gets \sigma + \alpha (\sigma_{\mathcal{B}} - \sigma)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter to tune so that the averages are
exponentially-decayed.  The result of normalizing <span class="math notranslate nohighlight">\(x\)</span> using the mini-batch
statistics is related to its moving averages by an affine transform</p>
<div class="math notranslate nohighlight">
\[\hat{x} \gets
\frac{x - \mu_{\mathcal{B}}}{\sigma_{\mathcal{B}}} r + d =
\frac{x - \mu}{\sigma}
\qquad \text{where} \qquad
r = \frac{\sigma_{\mathcal{B}}}{\sigma},
d = \frac{\mu_{\mathcal{B}} - \mu}{\sigma}.\]</div>
<p>Batch normalization is a special case that sets <span class="math notranslate nohighlight">\(r = 1\)</span> and <span class="math notranslate nohighlight">\(d = 0\)</span>.
Renormalization treats <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(d\)</span> as hyperparameters to tune.  One
scheme used in <a class="bibtex reference internal" href="#ioffe2017batch" id="id10">[Iof17]</a> defines</p>
<div class="math notranslate nohighlight">
\[\begin{split}\DeclareMathOperator{\stopgrad}{\text{stop_gradient}}
\newcommand{\clip}[2][]{\mathop{\mathrm{clip}_{#1}}\left( #2 \right)}
r &amp;\gets \stopgrad\left(
           \clip[1 / r_\max, r_\max]{\frac{\sigma_{\mathcal{B}}}{\sigma}}
         \right)\\
d &amp;\gets \stopgrad\left(
           \clip[-d_\max, d_\max]{\frac{\mu_{\mathcal{B}} - \mu}{\sigma}}
         \right).\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(r_\max = 1\)</span> and <span class="math notranslate nohighlight">\(d_\max = 0\)</span> initially, and each has its own
non-decreasing relaxation akin to the learning rate schedule.  The gradients
with respect to the parameters of batch renormalization are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial \sigma_{\mathcal{B}}} &amp;=
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               \frac{\partial \hat{x}_i}{\partial \sigma_{\mathcal{B}}} =
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               (x_i - \mu_{\mathcal{B}}) \frac{-r}{\sigma_{\mathcal{B}}^2}
\\\\
\frac{\partial l}{\partial \mu_{\mathcal{B}}} &amp;=
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               \frac{\partial \hat{x}_i}{\partial \mu_{\mathcal{B}}} =
\sum_{i = 1}^m \frac{\partial l}{\partial \hat{x}_i}
               \frac{-r}{\sigma_{\mathcal{B}}}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial l}{\partial x_i}
 &amp;= \sum_{j = 1}^m
      \frac{\partial l}{\partial \hat{x}_j}
      \left[
        \left( \mathbb{I}_i(j) - \frac{1}{m} \right)
          \frac{r}{\sigma_{\mathcal{B}}} +
        (x_j - \mu_{\mathcal{B}})
          \frac{-r}{\sigma_{\mathcal{B}}^2} \frac{1}{2 \sigma_{\mathcal{B}}}
          \frac{2}{m} (x_i - \mu_{\mathcal{B}})
      \right]\\
 &amp;= \frac{\partial l}{\partial \hat{x}_i} \frac{r}{\sigma_{\mathcal{B}}} +
    \frac{\partial l}{\partial \mu_{\mathcal{B}}} \frac{1}{m} +
    \frac{\partial l}{\partial \sigma_{\mathcal{B}}}
      \frac{x_i - \mu_{\mathcal{B}}}{m \sigma_{\mathcal{B}}}\end{split}\]</div>
<p>The other gradients are the same as batch normalization, and the inference step
now uses the moving averages instead of the population statistics.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/12/13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift-0"><dl class="citation">
<dt class="bibtex label" id="gitman2017comparison"><span class="brackets"><a class="fn-backref" href="#id6">GG17</a></span></dt>
<dd><p>Igor Gitman and Boris Ginsburg. Comparison of batch normalization and weight normalization algorithms for the large-scale image classification. <em>arXiv preprint arXiv:1709.08145</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="hyvarinen2000independent"><span class="brackets"><a class="fn-backref" href="#id9">HyvarinenO00</a></span></dt>
<dd><p>Aapo Hyvärinen and Erkki Oja. Independent component analysis: algorithms and applications. <em>Neural networks</em>, 13(4):411–430, 2000.</p>
</dd>
<dt class="bibtex label" id="ioffe2017batch"><span class="brackets">Iof17</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>,<a href="#id10">3</a>)</span></dt>
<dd><p>Sergey Ioffe. Batch renormalization: towards reducing minibatch dependence in batch-normalized models. <em>arXiv preprint arXiv:1702.03275</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="ioffe2015batch"><span class="brackets"><a class="fn-backref" href="#id4">IS15</a></span></dt>
<dd><p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In <em>International Conference on Machine Learning</em>, 448–456. 2015.</p>
</dd>
<dt class="bibtex label" id="mlugos134f2011"><span class="brackets"><a class="fn-backref" href="#id8">Lug</a></span></dt>
<dd><p>Michael Lugo. A proof of the central limit theorem. <span><a class="reference external" href="#"></a></span>https://www.stat.berkeley.edu/ mlugo/stat134-f11/clt-proof.pdf. Accessed on 2017-12-25.</p>
</dd>
<dt class="bibtex label" id="salimans2016weight"><span class="brackets">SK16</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Tim Salimans and Diederik P Kingma. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In <em>Advances in Neural Information Processing Systems</em>, 901–909. 2016.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/12/13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>