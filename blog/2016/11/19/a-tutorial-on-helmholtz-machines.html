<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>A Tutorial on Helmholtz Machines &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="When Do Stop-Loss Rules Stop Losses?" href="../18/when-do-stop-loss-rules-stop-losses.html" />
    <link rel="prev" title="Easy Volatility Investing" href="../20/easy-volatility-investing.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../12/20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.html">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.html">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/14/layer-normalization.html">Layer Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/08/a-fast-learning-algorithm-for-deep-belief-nets.html">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23/training-products-of-experts-by-minimizing-contrastive-divergence.html">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/27/sphinx-on-github-pages.html">Sphinx on GitHub Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">A Tutorial on Helmholtz Machines</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
<li><a class="reference internal" href="#notes">Notes</a><ul>
<li><a class="reference internal" href="#notations">Notations</a></li>
<li><a class="reference internal" href="#energy">Energy</a></li>
<li><a class="reference internal" href="#free-energy">Free Energy</a></li>
<li><a class="reference internal" href="#variational-free-energy">Variational Free Energy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../20/easy-volatility-investing.html" title="Previous Chapter: Easy Volatility Investing"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Easy Volatili...</span>
    </a>
  </li>
  <li>
    <a href="../18/when-do-stop-loss-rules-stop-losses.html" title="Next Chapter: When Do Stop-Loss Rules Stop Losses?"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">When Do Stop-... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="a-tutorial-on-helmholtz-machines">
<h1>A Tutorial on Helmholtz Machines<a class="headerlink" href="#a-tutorial-on-helmholtz-machines" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>The goal is to modernize the notations and clarify the concepts presented in
<a class="bibtex reference internal" href="#hinton1995wake" id="id1">[HDFN95]</a>, which assumes some familiarity with variational free
energy.</p>
<p>Supervised learning algorithms for multilayer neural networks require a teacher
to specify the desired output of the network and some method of communicating
error information to all of the connections.  When there is no external teaching
signal, some other goal is required to force the hidden units to extract the
underlying structure.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p><a class="bibtex reference internal" href="#hinton1995wake" id="id2">[HDFN95]</a> proposes the concept of a Helmholtz machine, a multilayer
neural network with generative and recognition weights.  It sees the world as
patterns of flickering bits, with each bit pattern
<span class="math notranslate nohighlight">\(\mathbf{d} \in \{0, 1\}^N\)</span> appearing with some probability distribution
<span class="math notranslate nohighlight">\(p(\mathbf{d})\)</span>.  In a completely random world, each bit in a random bit
vector can be assumed to be an independent and identically distributed random
variable.  This means the random bit vector can be described as a joint
distribution</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{d}) =
p(d_1, d_2, \ldots, d_N) =
\prod_i p(d_i) =
\prod_i p_0^{d_i} (1 - p_0)^{1 - d_i}\]</div>
<p>where the probability value <span class="math notranslate nohighlight">\(p_0 \in [0, 1]\)</span> is unknown.  To specify
<span class="math notranslate nohighlight">\(p(\mathbf{d})\)</span>, <span class="math notranslate nohighlight">\(2^N\)</span> bits are needed.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_G =
\{ \mathbf{b}, \mathbf{W}^1_G, \ldots, \mathbf{W}^L_G \}\)</span> denote the set of
model parameters to learn.  Let <span class="math notranslate nohighlight">\(\mathcal{M}_{\boldsymbol{\theta}_G}
\equiv \mathcal{M}(\boldsymbol{\theta}_G) =
\{ \mathbf{h}^1, \ldots, \mathbf{h}^L \}\)</span> represent the proposed neural network
model.  The Helmholtz machine attempts to find the minimum description length of
the data assuming <span class="math notranslate nohighlight">\(p(\mathbf{d})\)</span> can be approximated with a generative
factorial distribution <span class="math notranslate nohighlight">\(G\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\mathbf{d}) \approx
p_G(\mathbf{d}) &amp;= \sum p_G(\mathbf{d}, \mathcal{M}_{\theta_G})\\
 &amp;= \sum p_G(\mathbf{d}, \mathbf{h}^1, \mathbf{h}^2, \ldots, \mathbf{h}^L)\\
 &amp;= \sum p_G(\mathbf{d} \mid \mathbf{h}^1) \cdot
      \prod_{l = 1}^{L - 1} p_G(\mathbf{h}^l \mid \mathbf{h}^{l + 1}) \cdot
      p_G(\mathbf{h}^L)\\
 &amp;= \sum_{\mathbf{h}^1} p_G(\mathbf{d} \mid \mathbf{h}^1) \cdots
      \sum_{\mathbf{h}^{l + 1}}
        p_G(\mathbf{h}^l \mid \mathbf{h}^{l + 1}) \cdots
      \sum_{\mathbf{h}^L}
        p_G(\mathbf{h}^{L - 1} \mid \mathbf{h}^L) p_G(\mathbf{h}^L)
    \quad &amp; \quad \text{variable elimination}\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_G(\mathbf{h}^L)
 &amp;= \prod_j^{n_L} p_G(h^L_j)^{h^L_j}
    \left[ 1 - p_G(h^L_j) \right]^{1 - h^L_j}
    \quad &amp; \quad \text{with } p_G(h^L_j) = \sigma(b_j),\\
p_G(\mathbf{h}^l \mid \mathbf{h}^{l + 1})
 &amp;= \prod_j^{n_l} p_G(h^l_j \mid \mathbf{h}^{l + 1})^{h^l_j}
    \left[ 1 - p_G(h^l_j \mid \mathbf{h}^{l + 1}) \right]^{1 - h^l_j}
    \quad &amp; \quad \text{with }
    p_G(h^l_j \mid \mathbf{h}^{l + 1}) =
      \sigma\left(
        \mathbf{W}^{l + 1}_G[j \colon]
        \begin{bmatrix} \mathbf{h}^{l + 1}\\ 1 \end{bmatrix}
      \right),\\
p_G(\mathbf{d} \mid \mathbf{h}^1)
 &amp;= \prod_i^N p_G(d_i \mid \mathbf{h}^1)^{d^i}
    \left[ 1 - p_G(d_i \mid \mathbf{h}^1) \right]^{1 - d_i}
    \quad &amp; \quad \text{with }
    p_G(d_i \mid \mathbf{h}^1) =
      \sigma\left(
        \mathbf{W}^1_G[j \colon]
        \begin{bmatrix} \mathbf{h}^1\\ 1 \end{bmatrix}
      \right),\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid activation function.  The net’s bottom layer
<span class="math notranslate nohighlight">\(\mathbf{d}\)</span> is called the data (visible) layer; the remaining layers
above are called hidden layers.  Generating a data pattern <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>
begins with stochastically sampling <span class="math notranslate nohighlight">\(p_G(\mathbf{h}^L)\)</span> to yield a sample
<span class="math notranslate nohighlight">\(\mathbf{h}^L\)</span>.  This is implemented by having a single input value equal
to one coming into each neuron <span class="math notranslate nohighlight">\(j\)</span> in the top layer, each with a bias
weight <span class="math notranslate nohighlight">\(b_j \in [0, 1]\)</span>.  Each layer below follows the same procedure with
the corresponding weights until a bit vector <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> is generated;
note that this procedure is known as ancestral sampling.</p>
<p>Minimizing the difference between <span class="math notranslate nohighlight">\(p(\mathbf{d})\)</span> and
<span class="math notranslate nohighlight">\(p_G(\mathbf{d})\)</span> can be expressed as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_\boldsymbol{\theta} p(\mathbf{D}) \parallel p_G(\mathbf{D})
 &amp;= \argmin_\boldsymbol{\theta} \sum_\mathbf{d}
      p(\mathbf{d}) \log \frac{p(\mathbf{d})}{p_G(\mathbf{d})}\\
 &amp;= \argmin_\boldsymbol{\theta} \left[
      \sum_\mathbf{d} p(\mathbf{d}) \log p(\mathbf{d}) -
      \sum_\mathbf{d} p(\mathbf{d}) \log p_G(\mathbf{d})
    \right]\\
 &amp;= \argmin_\boldsymbol{\theta}
      -H(\mathbf{D}) +
      \langle -\log p_G(\mathbf{D}) \rangle\\
 &amp;= \argmin_\boldsymbol{\theta} \langle -\log p_G(\mathbf{D}) \rangle.\end{split}\]</div>
<p>While it is natural to minimize the expected surprise by applying
<a class="reference internal" href="../08/stochastic-gradient-descent-tricks.html"><span class="doc">stochastic gradient descent</span></a>
to evaluate</p>
<div class="math notranslate nohighlight">
\[\nabla \langle -\log p_G(\mathbf{D}) \rangle =
\sum_\mathbf{d} p(\mathbf{d}) \nabla \left[ -\log p_G(\mathbf{d}) \right] =
\sum_\mathbf{d} p(\mathbf{d}) \nabla \left[ F_G(\mathbf{d}) \right],\]</div>
<p>deriving the change in free energy with respect to the model parameters
<span class="math notranslate nohighlight">\(\frac{\partial F_G(\mathbf{d})}{\partial \theta}\)</span> is non-trivial.  One
alternative optimization strategy is to minimize the variational free energy
using a separate recognition distribution <span class="math notranslate nohighlight">\(R\)</span> such that</p>
<div class="math notranslate nohighlight">
\[p_R(\mathcal{M}_{\boldsymbol{\theta}_R}, \mid \mathbf{d}) =
p_R(\mathbf{h}^1, \mathbf{h}^2, \ldots, \mathbf{h}^L \mid \mathbf{d}) =
p_R(\mathbf{h}^1 \mid \mathbf{d})
  \prod_{l = 1}^{L - 1} p_R(\mathbf{h}^{l + 1} \mid \mathbf{h}^l)\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_R =
\{ \mathbf{W}^1_R, \ldots, \mathbf{W}^L_R \}\)</span> and</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_R(\mathbf{h}^{l + 1} \mid \mathbf{h}^l)
 &amp;= \prod_j^{n_{l + 1}} p_R(h^{l + 1}_j \mid \mathbf{h}^l)^{h^{l + 1}_j}
    \left[ 1 - p_R(h^{l + 1}_j \mid \mathbf{h}^l) \right]^{1 - h^{l + 1}_j}
    \quad &amp; \quad \text{with }
    p_R(h^{l + 1}_j \mid \mathbf{h}^l) =
      \sigma\left(
        \mathbf{W}^{l + 1}_R[j \colon]
        \begin{bmatrix} \mathbf{h}^l\\ 1 \end{bmatrix}
      \right),\\
p_R(\mathbf{h}^1 \mid \mathbf{d})
 &amp;= \prod_j^{n_1} p_R(h^1_j \mid \mathbf{d})^{h^1_j}
    \left[ 1 - p_R(h^1_j \mid \mathbf{d}) \right]^{1 - h^1_j}
    \quad &amp; \quad \text{with }
    p_R(h^1_j \mid \mathbf{d}) =
      \sigma\left(
        \mathbf{W}^1_R[j \colon]
        \begin{bmatrix} \mathbf{d}\\ 1 \end{bmatrix}
      \right).\end{split}\]</div>
<p>Each iteration in this revised gradient descent algorithm involves two phases.
The wake phase uses the recognition weights to minimize the variational free
energy <span class="math notranslate nohighlight">\(F_G^R\)</span> with respect to <span class="math notranslate nohighlight">\(G\)</span> using real world data.  The sleep
phase uses the generative weights to minimize</p>
<div class="math notranslate nohighlight">
\[p_G(\mathcal{M} \mid \mathbf{d}) \parallel p_R(\mathcal{M} \mid \mathbf{d})\]</div>
<p>with respect to <span class="math notranslate nohighlight">\(R\)</span> using stochastically sampled data.  Starting the
optimization with the wake phase and then alternating between wake and sleep
will make the recognition and generative models approximate inverses of each
other because they are optimizing in some sense the symmetrised KL divergence.</p>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>The tutorial generalizes the binary classification example presented in
<a class="bibtex reference internal" href="#hinton1995wake" id="id3">[HDFN95]</a>.  The world consists of <span class="math notranslate nohighlight">\(3 \times 3\)</span> images of
vertical and horizontal bars; the former occurs with twice the probability of
the latter, but within each category each configuration occurs with the same
probability.</p>
<p>The author simulates a 1-6-9 Helmholtz machine in the proposed world.  Each
layer has a different learning rate, and the weights were initialized to zero to
give each neuron a 50-50 chance of firing regardless of its input.</p>
<p>The results indicate the machine is successful at capturing much of this
world’s structure.  However, minimizing the KL divergence does not guarantee the
machine will learn sensible distributions for regions of no data.</p>
<p>Since the wake-sleep algorithm is unsupervised, it will needlessly expend
resources on regions that are of no interest.  Furthermore, the gradient of the
variational free energy may lead to incorrect mode-averaging.</p>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Would modeling each unit in a neural network as a (Gaussian) stochastic
process speed up learning?</p></li>
<li><p>Is it worthwhile to replace the variational method with automatic
differentiation to evaluate the desired free energy?</p></li>
<li><p>Would the Metropolis-Hastings algorithm speed up convergence?</p></li>
<li><p>Since neural networks are <a class="reference internal" href="../05/multilayer-feedforward-networks-are-universal-approximators.html"><span class="doc">universal approximators</span></a>,
would the network learn anything useful if each hidden layer
(or at the least the last layer) is connected to the data layer?  Use the
data at both ends of the neural network and apply backpropagation with
automatic differentiation for gradients to remove the factorial assumption.</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The sigmoid activation function converts the output of a real-valued neuron to
the probability that a binary-valued neuron fires; have neuroscience confirmed
that real neurons only care about binary outcomes?</p></li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Helmholtz machines alternately maximizes
<span class="math notranslate nohighlight">\(\langle \log p_G(\mathcal{M}, \mathbf{d}) \rangle_R\)</span> and
<span class="math notranslate nohighlight">\(\langle \log p_R(\mathcal{M} \mid \mathbf{d}) \rangle_G\)</span>.</p>
<p>The most blatant issue of <a class="bibtex reference internal" href="#hinton1995wake" id="id4">[HDFN95]</a> is its muddled exposition.
This tutorial effectively presents much more insights into Helmholtz machines
than one can possibly extract from <a class="bibtex reference internal" href="#hinton1995wake" id="id5">[HDFN95]</a>.  The connections
between energy, free energy, and variational free energy to a probabilistic
model are beautifully derived.  The derivation of the wake-sleep algorithm will
certainly be useful in future unsupervised learning research.  The tutorial
would be even better if the author explored other <a class="reference internal" href="../09/efficient-backprop.html"><span class="doc">initialization schemes</span></a>
to see if the machine dreams up more monsters.</p>
<p>Even though the machine is capable of capturing the data distribution, it is
still unclear how to extract meaningful relationships beyond probabilities.
This uncertainty casts some doubt on Karl Friston’s belief that the brain works
in a similar manner to the wake-sleep algorithm.</p>
</div>
<div class="section" id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="notations">
<h3>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>It is often convenient to recast a probability value <span class="math notranslate nohighlight">\(p \in [0, 1]\)</span> into
a quantity called surprise <span class="math notranslate nohighlight">\(s \equiv -\log p\)</span>.</p></li>
<li><p>The expected value of the surprise is called entropy:</p>
<div class="math notranslate nohighlight">
\[H(\mathbf{X}) = \mathbb{E}\left[ s(\mathbf{X}) \right] =
-\sum_{\mathbf{x} \in \mathbf{X}} p(\mathbf{x}) \log p(\mathbf{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a discrete random variable.</p>
</li>
<li><p>The conditional entropy is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}H(\mathbf{Y} \mid \mathbf{X})
 &amp;= \sum_{\mathbf{x} \in \mathbf{X}}
      p(\mathbf{x}) H(\mathbf{Y} \mid \mathbf{x})\\
 &amp;= \sum_{\mathbf{x} \in \mathbf{X}}
      p(\mathbf{x}) \left(
        -\sum_{\mathbf{y} \in \mathbf{Y}}
          p(\mathbf{y} \mid \mathbf{x}) \log p(\mathbf{y} \mid \mathbf{x})
      \right)\\
 &amp;= -\sum_{\mathbf{x} \in \mathbf{X}, \mathbf{y} \in \mathbf{Y}}
      p(\mathbf{x}, \mathbf{y}) \log p(\mathbf{y} \mid \mathbf{x}).\end{split}\]</div>
</li>
<li><p>Given two probability distributions <span class="math notranslate nohighlight">\(p_A(\mathbf{d})\)</span> and
<span class="math notranslate nohighlight">\(p_B(\mathbf{d})\)</span>, the relative entropy is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_A(\mathbf{D}) \parallel p_B(\mathbf{D})
 &amp;= \sum_{\mathbf{d} \in \mathbf{D}}
      p_A(\mathbf{d}) \log \frac{p_A(\mathbf{d})}{p_B(\mathbf{d})}\\
 &amp;= -H(A) -
    \sum_{\mathbf{d} \in \mathbf{D}} p_A(\mathbf{d}) \log p_B(\mathbf{d})\\
 &amp;= -H(A) +
    \sum_{\mathbf{d} \in \mathbf{D}}
      p_A(\mathbf{d}) \log \frac{1}{p_B(\mathbf{d})}\\
 &amp;= H(A, B) - H(A)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(H(A, B)\)</span> is known as the cross entropy.</p>
</li>
</ul>
</div>
<div class="section" id="energy">
<h3>Energy<a class="headerlink" href="#energy" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Suppose the state of a physical system fluctuates among a set of states
<span class="math notranslate nohighlight">\(\{ q_1, q_2, \ldots \}\)</span>.  One sign that the system is in thermal
equilibrium is that the probability of finding the system in a state
<span class="math notranslate nohighlight">\(q_i\)</span> is related to its energy <span class="math notranslate nohighlight">\(E(q_i)\)</span> according to the
Boltzmann distribution</p>
<div class="math notranslate nohighlight">
\[p(q_i) = \frac{\exp -\frac{E(q_i)}{T}}{\sum_j \exp -\frac{E(q_j)}{T}}\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the temperature.</p>
<ul class="simple">
<li><p>The denominator is known as the partition function <span class="math notranslate nohighlight">\(Z\)</span> and is often
the ultimate intractable quantity of interest.</p></li>
</ul>
</li>
<li><p>In non-physical systems, it may be desirable to work backwards and devise an
“energy” function so that the probability distribution over the states obey
a Boltzmann distribution.</p>
<ul class="simple">
<li><p>This enables one to employ theorems and relationships from statistical
physics.</p></li>
</ul>
</li>
<li><p>Given a fixed piece of data <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> and a generative distribution
<span class="math notranslate nohighlight">\(G\)</span> modeled as
<span class="math notranslate nohighlight">\(\mathcal{M}(\boldsymbol{\theta}) \equiv \mathcal{M}\)</span>, the likelihood</p>
<div class="math notranslate nohighlight">
\[p_G(\mathcal{M} \mid \mathbf{d}) =
\frac{p_G(\mathcal{M}, \mathbf{d})}{p_G(\mathbf{d})} =
\frac{p_G(\mathcal{M}, \mathbf{d})}{
  \sum_\mathcal{M} p_G(\mathcal{M}, \mathbf{d})
}\]</div>
<p>can be reformulated as</p>
<div class="math notranslate nohighlight">
\[p_G(\mathcal{M} \mid \mathbf{d}) =
\frac{\exp -E_G(\mathcal{M}; \mathbf{d})}{
  \sum_\mathcal{M} \exp -E_G(\mathcal{M}; \mathbf{d})
}\]</div>
<p>where <span class="math notranslate nohighlight">\(E_G(\mathcal{M}; \mathbf{d})
\equiv -\log p_G(\mathcal{M}, \mathbf{d})\)</span> is the energy of the explanation
<span class="math notranslate nohighlight">\(\mathcal{M}\)</span> of the data pattern <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>.</p>
<ul class="simple">
<li><p>The energy is the surprise associated with the occurrence of a particular
complete state.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="free-energy">
<h3>Free Energy<a class="headerlink" href="#free-energy" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>The Helmholtz free energy of a system with average energy
<span class="math notranslate nohighlight">\(\langle E \rangle\)</span>, temperature <span class="math notranslate nohighlight">\(T\)</span>, and entropy <span class="math notranslate nohighlight">\(H\)</span> is
given by</p>
<div class="math notranslate nohighlight">
\[F = \langle E \rangle - TH.\]</div>
</li>
<li><p>Given a fixed piece of data <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> and a generative distribution
<span class="math notranslate nohighlight">\(G\)</span> modeled as
<span class="math notranslate nohighlight">\(\mathcal{M}(\boldsymbol{\theta}) \equiv \mathcal{M}\)</span>, the surprise of
<span class="math notranslate nohighlight">\(\mathbf{d}\)</span> in <span class="math notranslate nohighlight">\(G\)</span> can be interpreted as the Helmholtz free
energy of <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> in <span class="math notranslate nohighlight">\(G\)</span> because</p>
<div class="math notranslate nohighlight">
\[\begin{split}-\log p_G(\mathbf{d})
 &amp;= -(1)\log p_G(\mathbf{d})\\
 &amp;= -\left( \sum_\mathcal{M} p_G(\mathcal{M} \mid \mathbf{d}) \right)
    \log p_G(\mathbf{d})\\
 &amp;= -\sum_\mathcal{M} p_G(\mathcal{M} \mid \mathbf{d})
    \log \frac{
      p_G(\mathcal{M}, \mathbf{d})
    }{
      p_G(\mathcal{M} \mid \mathbf{d})
    }\\
 &amp;= -\sum_\mathcal{M}
      p_G(\mathcal{M} \mid \mathbf{d}) \log p_G(\mathcal{M}, \mathbf{d}) -
      p_G(\mathcal{M} \mid \mathbf{d})
        \log p_G(\mathcal{M} \mid \mathbf{d})\\
 &amp;= \langle E_G(\mathcal{M}; \mathbf{d}) \rangle_G -
    H_G(\mathcal{M} \mid \mathbf{d})\\
 &amp;= F_G(\mathbf{d}).\end{split}\]</div>
<ul class="simple">
<li><p>The first term is the energy of explanations of <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> weighted
by the distribution <span class="math notranslate nohighlight">\(G\)</span>.</p></li>
<li><p>The second term is the entropy of explanations of <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>
weighted by the distribution <span class="math notranslate nohighlight">\(G\)</span>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="variational-free-energy">
<h3>Variational Free Energy<a class="headerlink" href="#variational-free-energy" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Given a fixed piece of data <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>, a generative distribution
<span class="math notranslate nohighlight">\(G\)</span> modeled as <span class="math notranslate nohighlight">\(\mathcal{M}_{\boldsymbol{\theta}_G}\)</span>, and another
arbitrary distribution <span class="math notranslate nohighlight">\(R\)</span> using
<span class="math notranslate nohighlight">\(\mathcal{M}_{\boldsymbol{\theta}_R}\)</span> (i.e. uses the same model but
different parameter estimates), the variational free energy
<span class="math notranslate nohighlight">\(F^R_G(\mathbf{d})\)</span> from <span class="math notranslate nohighlight">\(R\)</span> to <span class="math notranslate nohighlight">\(G\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_R(\mathcal{M} \mid \mathbf{d}) \parallel p_G(\mathcal{M} \mid \mathbf{d})
 &amp;= \sum_\mathcal{M} p_R(\mathcal{M} \mid \mathbf{d})
    \log \frac{
      p_R(\mathcal{M} \mid \mathbf{d})
    }{
      p_G(\mathcal{M} \mid \mathbf{d})
    }\\
 &amp;= \sum_\mathcal{M} p_R(\mathcal{M} \mid \mathbf{d})
      \log p_R(\mathcal{M} \mid \mathbf{d}) -
    \sum_\mathcal{M}
      p_R(\mathcal{M} \mid \mathbf{d}) \log p_G(\mathcal{M}, \mathbf{d}) +
    \sum_\mathcal{M}
      p_R(\mathcal{M} \mid \mathbf{d}) \log p_G(\mathbf{d})\\
 &amp;= -H_R(\mathcal{M} \mid \mathbf{d}) +
    \sum_\mathcal{M}
      p_R(\mathcal{M} \mid \mathbf{d}) E_G(\mathcal{M}; \mathbf{d}) +
    \log p_G(\mathbf{d})
      \sum_\mathcal{M} p_R(\mathcal{M} \mid \mathbf{d})\\
 &amp;= -H_R(\mathcal{M} \mid \mathbf{d}) +
    \langle E_G(\mathcal{M}; \mathbf{d}) \rangle_R -
    F_G(\mathbf{d})\\
 &amp;= F^R_G(\mathbf{d}) - F_G(\mathbf{d}).\end{split}\]</div>
<ul>
<li><p>Since KL divergences are never negative,</p>
<div class="math notranslate nohighlight">
\[F^R_G(\mathbf{d}) \equiv
\langle E_G(\mathcal{M}; \mathbf{d}) \rangle_R -
  H_R(\mathcal{M} \mid \mathbf{d}) \geq F_G(\mathbf{d}).\]</div>
</li>
<li><p>Notice that minimizing the variational free energy on both <span class="math notranslate nohighlight">\(R\)</span> and
<span class="math notranslate nohighlight">\(G\)</span> results in <span class="math notranslate nohighlight">\(p_R \rightarrow p_G\)</span> and <span class="math notranslate nohighlight">\(F_G(\mathbf{d})\)</span>
being minimized.</p></li>
<li><p>Observe that the Kullback-Leibler divergence is not symmetric because</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}p_G(\mathcal{M} \mid \mathbf{d}) \parallel p_R(\mathcal{M} \mid \mathbf{d})
 &amp;= \sum_\mathcal{M} p_G(\mathcal{M} \mid \mathbf{d})
    \log \frac{
      p_G(\mathcal{M} \mid \mathbf{d})
    }{
      p_R(\mathcal{M} \mid \mathbf{d})
    }\\
 &amp;= \left\langle
      -\log p_R(\mathcal{M} \mid \mathbf{d})
    \right\rangle_G -
    H_G(\mathcal{M} \mid \mathbf{d}).\end{split}\]</div>
</li>
</ul>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/11/19/a-tutorial-on-helmholtz-machines-0"><dl class="citation">
<dt class="bibtex label" id="hinton1995wake"><span class="brackets">HDFN95</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>,<a href="#id4">4</a>,<a href="#id5">5</a>)</span></dt>
<dd><p>Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The “wake-sleep” algorithm for unsupervised neural networks. <em>Science</em>, 268(5214):1158, 1995.</p>
</dd>
<dt class="bibtex label" id="kirby2006tutorial"><span class="brackets">Kir06</span></dt>
<dd><p>Kevin G Kirby. A tutorial on helmholtz machines. 2006.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/11/19/a-tutorial-on-helmholtz-machines.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>