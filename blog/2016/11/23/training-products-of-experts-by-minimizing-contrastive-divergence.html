<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Training Products of Experts by Minimizing Contrastive Divergence &#8212; All Things Phi</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/my-styles.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../../_static/phi.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Market Timing with Candlestick Technical Analysis" href="../22/market-timing-with-candlestick-technical-analysis.html" />
    <link rel="prev" title="Neural Networks and Physical Systems with Emergent Collective Computational Abilities" href="../24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          All Things Phi</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Archive <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../12/20/mask-r-cnn.html">Mask R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/19/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.html">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/18/watertight-ray-triangle-intersection.html">Watertight Ray/Triangle Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/17/understanding-deep-learning-requires-rethinking-generalization.html">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/16/accurate-large-minibatch-sgd-training-imagenet-in-1-hour.html">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/15/on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/14/layer-normalization.html">Layer Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/13/batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/12/deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/11/optimal-step-nonrigid-icp-algorithms-for-surface-registration.html">Optimal Step Nonrigid ICP Algorithms for Surface Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/10/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.html">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/09/least-squares-estimation-of-transformation-parameters-between-two-point-patterns.html">Least-Squares Estimation of Transformation Parameters Between Two Point Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/08/a-fast-learning-algorithm-for-deep-belief-nets.html">A Fast Learning Algorithm for Deep Belief Nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/07/a-view-of-the-EM-algorithm-that-justifies-incremental-sparse-and-other-variants.html">A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/06/one-weird-trick-for-parallelizing-convolutional-neural-networks.html">One Weird Trick for Parallelizing Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/05/exponential-family-harmoniums-with-an-application-to-information-retrieval.html">Exponential Family Harmoniums with an Application to Information Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/04/pose-space-deformation-a-unified-approach-to-shape-interpolation-and-skeleton-driven-deformation.html">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/02/learning-internal-representations-by-error-propagation.html">Learning Internal Representations by Error Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12/01/structuring-a-renderer-phi-ray.html">Structuring a Renderer: <span class="math notranslate nohighlight">\(\varphi\)</span>-Ray</a></li>
<li class="toctree-l1"><a class="reference internal" href="../30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html">Information Processing in Dynamical Systems: Foundations of Harmony Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../29/an-introduction-to-the-conjugate-gradient-method-without-the-agonizing-pain.html">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../28/a-learning-algorithm-for-boltzmann-machines.html">A Learning Algorithm for Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../27/geometric-skinning-with-approximate-dual-quaternion-blending.html">Geometric Skinning with Approximate Dual Quaternion Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../26/the-perceptron-a-probabilistic-model-for-information-storage-and-organization-in-the-brain.html">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../25/the-sharpe-ratio.html">The Sharpe Ratio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html">Neural Networks and Physical Systems with Emergent Collective Computational Abilities</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training Products of Experts by Minimizing Contrastive Divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22/market-timing-with-candlestick-technical-analysis.html">Market Timing with Candlestick Technical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21/all-that-glitters-is-not-gold-comparing-backtest-and-out-of-sample-performance-on-a-large-cohort-of-trading-algorithms.html">All that Glitters is Not Gold: Comparing Backtest and Out-of-Sample Performance on a Large Cohort of Trading Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../20/easy-volatility-investing.html">Easy Volatility Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../19/a-tutorial-on-helmholtz-machines.html">A Tutorial on Helmholtz Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18/when-do-stop-loss-rules-stop-losses.html">When Do Stop-Loss Rules Stop Losses?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17/a-simple-implicit-measure-of-the-effective-bid-ask-spread-in-an-efficient-market.html">A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16/asset-prices-and-trading-volume-under-fixed-transactions-costs.html">Asset Prices and Trading Volume Under Fixed Transactions Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15/maxout-networks.html">Maxout Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13/dropout-training-as-adaptive-regularization.html">Dropout Training as Adaptive Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12/model-compression.html">Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11/distilling-the-knowledge-in-a-neural-network.html">Distilling the Knowledge in a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10/do-deep-nets-really-need-to-be-deep.html">Do Deep Nets Really Need to be Deep?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09/efficient-backprop.html">Efficient Backprop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08/stochastic-gradient-descent-tricks.html">Stochastic Gradient Descent Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07/automatic-differentiation-in-machine-learning-a-survey.html">Automatic Differentiation in Machine Learning: A Survey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06/econometric-models-of-limit-order-executions.html">Econometric Models of Limit-Order Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05/multilayer-feedforward-networks-are-universal-approximators.html">Multilayer Feedforward Networks are Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04/dendritic-computation.html">Dendritic Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03/understanding-order-flow.html">Understanding Order Flow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02/optimal-control-of-execution-costs.html">Optimal Control of Execution Costs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01/risks-and-portfolio-decisions-involving-hedge-funds.html">Risks and Portfolio Decisions Involving Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/30/coordinate-systems.html">Coordinate Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/28/hedge-funds-the-living-and-the-dead.html">Hedge Funds: The Living and the Dead</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/24/do-hedge-funds-have-enough-capital-a-value-at-risk-approach.html">Do Hedge Funds Have Enough Capital?  A Value-at-Risk Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/23/characterizing-computer-performance-with-a-single-number.html">Characterizing Computer Performance with a Single Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/22/how-to-not-lie-with-statistics-the-correct-way-to-summarize-benchmark-results.html">How Not to Lie with Statistics: The Correct Way to Summarize Benchmark Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/19/an-econometric-analysis-of-serial-correlation-and-illiquidity-in-hedge-fund-returns.html">An Econometric Analysis of Serial Correlation and Illiquidity in Hedge-Fund Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/13/empirical-characteristics-of-dynamic-trading-strategies-the-case-of-hedge-funds.html">Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/06/orange-juice-and-weath.html">Orange Juice and Weather</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/04/the-adaptive-markets-hypothesis-market-efficiency-from-an-evolutionary-perspective.html">The Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10/02/do-asset-prices-reflect-fundamentals-freshly-squeezed-evidence-from-the-oj-market.html">Do Asset Prices Reflect Fundamentals?  Freshly Squeezed Evidence from the OJ Market</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/30/neuroeconomics-how-neuroscience-can-inform-economics.html">Neuroeconomics: How Neuroscience Can Inform Economics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/28/drawing-inferences-from-statistics-based-on-multiyear-asset-returns.html">Drawing Inferences from Statistics based on Multiyear Asset Returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/26/when-are-contrarian-profits-due-to-stock-market-overreaction.html">When are Contrarian Profits Due to Stock Market Overreaction?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/24/profitability-of-momentum-strategies-an-evaluation-of-alternative-explanations.html">Profitability of Momentum Strategies: An Evaluation of Alternative Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/22/the-restrictions-on-predictability-implied-by-rational-asset-pricing.html">The Restrictions on Predictability Implied by Rational Asset Pricing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/20/the-myth-of-long-horizon-predictability.html">The Myth of Long-Horizon Predictability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/18/the-standard-error-of-regressions.html">The Standard Error of Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/16/lets-take-the-con-out-of-econometrics.html">Let’s Take the Con Out of Econometrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/14/role-of-models-in-statistical-analysis.html">Role of Models in Statistical Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09/12/the-experimental-generation-of-interpersonal-closeness-a-procedure-and-some-preliminary-findings.html">The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/02/notes-on-tensorflow.html">Notes on TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08/01/tensorflow-tensorboard-and-docker.html">TensorFlow, TensorBoard, and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/13/review-and-analysis-of-solutions-of-the-three-point-perspective-pose-estimation-problem.html">Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/12/variational-learning-for-switching-state-space-models.html">Variational Learning for Switching State-Space Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/11/bayesian-face-recognition.html">Bayesian Face Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/10/robust-generative-subspace-modeling-the-subspace-t-distribution.html">Robust Generative Subspace Modeling: The Subspace <span class="math notranslate nohighlight">\(t\)</span> Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/09/robust-subspace-mixture-models-using-t-distributions.html">Robust Subspace Mixture Models using <span class="math notranslate nohighlight">\(t\)</span>-distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/08/robust-mixture-modelling-using-the-t-distribution.html">Robust Mixture Modelling using the <span class="math notranslate nohighlight">\(t\)</span>-distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/07/mixtures-of-probabilistic-principal-component-analyzers.html">Mixtures of Probabilistic Principal Component Analysers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/06/the-em-algorithm-for-mixtures-of-factor-analyzers.html">The EM Algorithm for Mixtures of Factor Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/05/3d-live-real-time-captured-content-for-mixed-reality.html">3D Live: Real Time Captured Content for Mixed Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/04/high-accuracy-stereo-depth-maps-using-structured-light.html">High-Accuracy Stereo Depth Maps Using Structured Light</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/03/simple-accurate-and-robust-projector-camera-calibration.html">Simple, Accurate, and Robust Projector-Camera Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/02/multiresolution-gray-scale-and-rotation-invariant-texture-classification-with-local-binary-patterns.html">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01/01/generative-or-discriminative-getting-the-best-of-both-worlds.html">Generative or Discriminative?  Getting the Best of Both Worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/02/01/seda-an-architecture-for-well-conditioned,-scalable-internet-services.html">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/26/reconciling-environment-integration-and-component-independence.html">Reconciling Environment Integration and Component Independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/25/design-patterns-abstraction-and-reuse-of-object-oriented-design.html">Design Patterns: Abstraction and Reuse of Object-Oriented Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/24/a-guide-to-metaphorical-design.html">A Guide to Metaphorical Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/23/a-spiral-model-of-software-development-and-enhancement.html">A Spiral Model of Software Development and Enhancement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/22/sequential-and-concurrent-object-oriented-programming.html">Sequential and Concurrent Object-Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/21/software-aging.html">Software Aging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/20/applying-design-by-contract.html">Applying “Design by Contract”</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/19/predicate-logic-for-software-engineering.html">Predicate Logic for Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/18/active-design-reviews-principles-and-practices.html">Active Design Reviews: Principles and Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/17/a-rational-design-process-how-and-why-to-fake-it.html">A Rational Design Process: How and Why to Fake It</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/16/the-modular-structure-of-complex-systems.html">The Modular Structure of Complex Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/15/abstract-types-defined-as-classes-of-variables.html">Abstract Types Defined as Classes of Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/14/use-of-abstract-interfaces-in-the-development-of-software-for-embedded-computer-systems.html">Use of Abstract Interfaces in the Development of Software for Embedded Computer Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/13/the-influence-of-software-structure-on-reliability.html">The Influence of Software Structure on Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/12/response-to-detected-errors-in-well-structured-programs.html">Response to Detected Errors in Well-Structured Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/11/the-use-of-abstract-data-types-to-simplify-program-modifications.html">The Use of Abstract Data Types to Simplify Program Modifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/10/use-of-the-concept-of-transparency-in-the-design-of-hierarchically-structured-systems.html">Use of the Concept of Transparency in the Design of Hierarchically Structured Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/09/designing-software-for-ease-of-extension-and-contraction.html">Designing Software for Ease of Extension and Contraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/08/on-the-design-and-development-of-program-families.html">On the Design and Development of Program Families</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/07/on-the-criteria-to-be-used-in-decomposing-systems-into-modules.html">On the Criteria to be Used in Decomposing Systems into Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/06/a-technique-for-software-module-specification-with-examples.html">A Technique for Software Module Specification with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/05/information-distribution-aspects-of-design-methodology.html">Information Distribution Aspects of Design Methodology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/04/a-model-of-large-program-development.html">A Model of Large Program Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/03/architectural-styles-and-the-design-of-network-based-software-architectures.html">Architectural Styles and the Design of Network-based Software Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/02/design-of-design.html">Design of Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2014/01/01/notes-on-the-synthesis-of-form.html">Notes on the Synthesis of Form</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/26/blogging-with-docker.html">Blogging with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/25/typical-mercurial-usage.html">Typical Mercurial Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/24/profiling-on-linux.html">Profiling on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/23/trading-cryptocurrencies.html">Trading Cryptocurrencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/22/notes-on-software-design.html">Notes on Software Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/21/notes-on-scraping-together-a-heterogeneous-system.html">Notes on Scraping Together a Heterogeneous System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/20/transfer-media-files-to-mobile-device-via-vlc.html">Transfer Media Files to Mobile Device via VLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/19/style-lessons-in-clarity-and-grace.html">Style: Lessons in Clarity and Grace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/18/the-science-of-scientific-writing.html">The Science of Scientific Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/17/collection-of-notes-on-research.html">Collection of Notes on Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/16/typical-ffmpeg-usage.html">Typical FFmpeg Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/15/generate-svg-graphics.html">Generate SVG Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/14/blogging-with-restructuredtext-a-google-domain-and-sphinx.html">Blogging with RestructuredText, a Google Domain, and Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/13/set-up-android-development-environment.html">Set Up Android Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/12/svegan-lifestyle.html">Svegan Lifestyle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/11/set-up-system-programming-environment.html">Set Up System Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/10/the-rise-and-fall-of-react-flux-redux-and-cycle.html">The Rise and Fall of React, Flux, Redux, and Cycle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/09/install-graphics-and-compute-linux-mint.html">Install Graphics and Compute on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/08/set-up-web-development-environment.html">Set Up Web Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/07/vfio-tips-and-tricks.html">VFIO Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/06/options-trading.html">Options Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/05/assimp-mesh-loader.html">Assimp Mesh Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/04/set-up-data-analysis-environment.html">Set Up Data Analysis Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/03/install-nvidia-drivers-on-linux-mint.html">Install Nvidia Drivers on Linux Mint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/02/blogging-with-restructuredtext-a-google-domain-and-pelican.html">Blogging with RestructuredText, a Google Domain, and Pelican</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../2013/01/01/linux-mint-installation.html">Linux Mint Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/an-invitation-to-3d-vision-msks/index.html">An Invitation to 3-D Vision - Ma, Soatto, Kosecka, and Sastry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/complete-musician-laitz/index.html">The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening - Laitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-science-theory-for-the-information-age-hk/index.html">Computer Science Theory for the Information Age - Hopcroft &amp; Kannan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/computer-vision-models-learning-and-inference-prince/index.html">Computer Vision: Models, Learning, and Inference - Prince</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/creativity-nlph/index.html">Creativity - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/differential-geometry-from-a-graphics-perspective-nlph/index.html">Differential Geometry from a Graphics Perspective - NLPH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/fundamentals-of-electric-circuits-as/index.html">Fundamentals of Electric Circuits - Alexander &amp; Sadiku</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/linear-programming-vanderbei/index.html">Linear Programming - Vanderbei</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/multiple-view-geometry-hz/index.html">Multiple View Geometry in Computer Vision - Hartley &amp; Zisserman</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/numerical-methods-for-unconstrained-optimization-and-nonlinear-equations-ds/index.html">Numerical Methods for Unconstrained Optimization and Nonlinear Equations - Dennis &amp; Schnabel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/pattern-recognition-and-machine-learning-bishop/index.html">Pattern Recognition and Machine Learning - Bishop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/reinforcement-learning-sb/index.html">Reinforcement Learning: An Introduction - Sutton &amp; Barto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nb/stat-labs-ns/index.html">Stat Labs - Nolan &amp; Speed</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Training Products of Experts by Minimizing Contrastive Divergence</a><ul>
<li><a class="reference internal" href="#motivation-s">Motivation(s)</a></li>
<li><a class="reference internal" href="#proposed-solution-s">Proposed Solution(s)</a></li>
<li><a class="reference internal" href="#evaluation-s">Evaluation(s)</a></li>
<li><a class="reference internal" href="#future-direction-s">Future Direction(s)</a></li>
<li><a class="reference internal" href="#question-s">Question(s)</a></li>
<li><a class="reference internal" href="#analysis">Analysis</a></li>
<li><a class="reference internal" href="#notes">Notes</a><ul>
<li><a class="reference internal" href="#maximum-likelihood-learning">Maximum Likelihood Learning</a></li>
<li><a class="reference internal" href="#relationship-between-maximum-likelihood-and-kullback-leibler-divergence">Relationship between Maximum Likelihood and Kullback-Leibler Divergence</a></li>
<li><a class="reference internal" href="#contrastive-divergence">Contrastive Divergence</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../24/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html" title="Previous Chapter: Neural Networks and Physical Systems with Emergent Collective Computational Abilities"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Neural Networ...</span>
    </a>
  </li>
  <li>
    <a href="../22/market-timing-with-candlestick-technical-analysis.html" title="Next Chapter: Market Timing with Candlestick Technical Analysis"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Market Timing... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="training-products-of-experts-by-minimizing-contrastive-divergence">
<h1>Training Products of Experts by Minimizing Contrastive Divergence<a class="headerlink" href="#training-products-of-experts-by-minimizing-contrastive-divergence" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-s">
<h2>Motivation(s)<a class="headerlink" href="#motivation-s" title="Permalink to this headline">¶</a></h2>
<p>Fitting a mixture model via EM or gradient ascent is one way to model a
complicated, smooth, high-dimensional data distribution.  One limitation is that
the posterior distribution cannot be sharper than the individual models in the
mixture.  This issue becomes more problematic in high-dimensional spaces where
the individual models need to be broadly tuned.</p>
</div>
<div class="section" id="proposed-solution-s">
<h2>Proposed Solution(s)<a class="headerlink" href="#proposed-solution-s" title="Permalink to this headline">¶</a></h2>
<p>The author proposes the concept of a Product of Experts (PoE)</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{d} \mid \theta_1, \ldots, \theta_n) =
\frac{\prod_m p_m(\mathbf{d} \mid \theta_m)}{
  \sum_\mathbf{c} \prod_m p_m(\mathbf{c} \mid \theta_m)
} =
Z^{-1} \prod_m p_m(\mathbf{d} \mid \theta_m)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> is a data vector in a discrete space, <span class="math notranslate nohighlight">\(\theta_m\)</span>
represents all the parameters of an individual model <span class="math notranslate nohighlight">\(m\)</span>,
<span class="math notranslate nohighlight">\(p_m(\mathbf{d} \mid \theta_m)\)</span> is the probability of <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>
under model <span class="math notranslate nohighlight">\(m\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> indexes all possible vectors in
the data space.  This enables each expert to specialize on a different subset
of the dimensions in a high-dimensional space.  Note that
<span class="math notranslate nohighlight">\(p_m(\mathbf{d} \mid \theta)\)</span> could be any non-negative function
<span class="math notranslate nohighlight">\(f(\mathbf{d}; \theta_m)\)</span> due to the partition function <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>Directly fitting a PoE to a set of observed i.i.d. data vectors requires solving</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial \theta_m}
    \log p(\mathbf{d} \mid \theta_1, \ldots, \theta_n)
 &amp;= \sum_m \frac{\partial}{\partial \theta_m}
        \log p_m(\mathbf{d} \mid \theta_m) -
    \frac{\partial}{\partial \theta_m} \log Z\\
 &amp;= \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m} -
    Z^{-1} \sum_\mathbf{c} \frac{\partial}{\partial \theta_m}
        p_m(\mathbf{c} \mid \theta_m)
        \prod_{m' \neq m} p_{m'}(\mathbf{c} \mid \theta_{m'})\\
 &amp;= \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m} -
    \sum_\mathbf{c}
      p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
      \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}.\end{split}\]</div>
<p>Since the hidden states of all the experts are conditionally independent given
the data, Gibbs sampling can update each of them in parallel.  Unfortunately,
samples drawn from the equilibrium distribution have very high variance because
they come from different parts of the model’s distribution.  Furthermore, the
variance in the samples depends on the parameters of the model causing the
parameters to be repelled from regions of high variance even if the gradient is
zero.</p>
<p>Ideally, the Markov chain that is implemented by Gibbs sampling should leave the
initial distribution over the visible variables unaltered.  Towards this goal of
reducing the variance, the author proposes the method of contrastive divergence:
instead of running the Markov chain to equilibrium, run the chain for one full
step and then update the parameters.</p>
</div>
<div class="section" id="evaluation-s">
<h2>Evaluation(s)<a class="headerlink" href="#evaluation-s" title="Permalink to this headline">¶</a></h2>
<p>The experiments on synthetic data (e.g. 5 x 5 clusters using 15 univariate
Gaussian experts, 100-dimensional images containing edges) indicate that a PoE
is able to fit data distributions that can be factorized into a product of lower
dimensional distributions.  The simulations reveal that separate initialization
and training causes PoE to become trapped in poor local optima.  A workaround is
to train the experts together with random initialization.</p>
<p>Using contrastive divergence with RBMs on the USPS digits dataset achieved an
error rate of 1.1%.  The weights were learned by doing multinomial logistic
regression on the training data with the labels as outputs and the unnormalised
log probability scores from the trained (digit-specific) PoE as inputs.  Note
that the USPS test set is drawn from a different distribution than the training
set.  To sidestep this issue, the author created a new test set from the unused
portion of the training data.</p>
<p>To get an idea of the relative magnitude of the ignored term in contrastive
divergence, extensive simulations were performed using RBMs with small
numbers of visible and hidden units.  RBMs with few units allow a brute force
evaluation of what would be exponential in the number of hidden/visible units.
The results indicate the learning procedure does not always improve the log
likelihood of the training data, though it has a strong tendency to do so.
The paramount point to realize is the approximation did not make contrastive
divergence worse in latter iterations.</p>
</div>
<div class="section" id="future-direction-s">
<h2>Future Direction(s)<a class="headerlink" href="#future-direction-s" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In Andrew Ng’s talk on <a class="reference download internal" download="" href="../../../../_downloads/e38d9782aa4834a571716f751e855991/nuts-and-bolts-of-building-ai-applications-using-deep-learning.pdf"><code class="xref download docutils literal notranslate"><span class="pre">Bias-Variance</span> <span class="pre">Tradeoff</span></code></a>,
the subject of different train and test data distributions was brought up
again.  The proposed solution is to craft an appropriate data distribution.
This is in stark contrast with how humans confront novel situations.  What is
a reasonable formulation of transfer learning that emphasizes the minimization
of self-contradiction?</p></li>
</ul>
</div>
<div class="section" id="question-s">
<h2>Question(s)<a class="headerlink" href="#question-s" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>Isn’t a learning algorithm rather fragile if the test and training data needs
to come from the same distribution?  If a human is recognizing a digit, it
doesn’t matter whether the digit is made of wood or water.</p></li>
<li><p>Have multinomial pixels and PoE HMM withstood the test of time?  They seem
overly complicated compared to existing techniques.</p></li>
<li><p>When is the relationship of equation (12) useful?</p>
<div class="math notranslate nohighlight">
\[P \parallel Z^{-1} \prod_m Q_m^{w_m} \leq \sum_m w_m P \parallel Q_m\]</div>
</li>
</ul>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Generative models that choose latent variables and then generate data tend to
have a strong tendency for the posterior values of the latent variables to be
approximately marginally independent after the model has been fitted to data.
This is why it’s hard to learn one hidden layer at a time in a greedy bottom-up
way.  With a PoE, even though the experts have independent priors, the latent
variables of different experts will be marginally dependent.  This means there
may still be lots of statistical structure in the latent variables for
additional hidden layers to capture.  Furthermore, a PoE retains the property
of orthogonal basis functions while allowing non-orthogonal experts and a
non-linear generative model.</p>
<p>In order to fully understand this paper, one should already be familiar with
<a class="reference internal" href="../28/a-learning-algorithm-for-boltzmann-machines.html"><span class="doc">Boltzmann machines</span></a>
and <a class="reference internal" href="../30/information-processing-in-dynamical-systems-foundations-of-harmony-theory.html"><span class="doc">RBMs</span></a>.
<a class="bibtex reference internal" href="#ojwnocd" id="id1">[Woo]</a><a class="bibtex reference internal" href="#hpcd" id="id2">[Puh]</a> present a concise and modern exposition on the application
of contrastive divergence.  Some additional insights may exist in the
experimental results of the original paper <a class="bibtex reference internal" href="#hinton2002training" id="id3">[Hin02]</a>.</p>
<p>One interesting tidbit is that an RBM is a PoE with one expert per hidden unit,
and RBMs can be considered as the intersection between Boltzmann machines and
PoE.  While a PoE is novel and possibly useful, CD-k enables sidestepping
intractable calculations with acceptable variance and bias.</p>
</div>
<div class="section" id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="maximum-likelihood-learning">
<h3>Maximum Likelihood Learning<a class="headerlink" href="#maximum-likelihood-learning" title="Permalink to this headline">¶</a></h3>
<p>Given a finite set of training data</p>
<div class="math notranslate nohighlight">
\[\mathcal{X} = \left\{ \mathbf{x}_i \right\}_{i = 1}^N \subset \mathcal{D},\]</div>
<p>one would like to model the probability of a data point <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>
using a non-negative function of the form <span class="math notranslate nohighlight">\(f(\mathbf{x}; \Theta)\)</span> where
<span class="math notranslate nohighlight">\(\Theta\)</span> is a vector of model parameters.  The corresponding likelihood
function is</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x} \mid \Theta) = \frac{1}{Z(\Theta)} f(\mathbf{x}; \Theta)\]</div>
<p>where the partition function <span class="math notranslate nohighlight">\(Z(\Theta)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[Z(\Theta) =
\int_{\mathbf{x} \in \mathcal{D}}
  f(\mathbf{x}; \Theta) \mathop{\mathrm{d}\mathbf{x}}.\]</div>
<p>The goal is to find the</p>
<div class="math notranslate nohighlight">
\[\begin{split}\DeclareMathOperator*{\argmax}{arg\,max}
\argmax_{\Theta} p(\mathcal{X} \mid \Theta)
 &amp;= \argmax_{\Theta} \prod_{i = i}^N p(\mathbf{x}_i \mid \Theta)
    &amp; \quad &amp; \text{observations are i.i.d.}\\
 &amp;= \argmax_{\Theta}
      \prod_{i = i}^N \frac{f(\mathbf{x}_i; \Theta)}{Z(\Theta)}\\
 &amp;= \argmax_{\Theta}
      N^{-1} \log \prod_{i = i}^N \frac{f(\mathbf{x}_i; \Theta)}{Z(\Theta)}\\
 &amp;= \argmax_{\Theta} -\log Z(\Theta) +
      \frac{1}{N} \sum_{i = i}^N \log f(\mathbf{x}_i; \Theta)\\
 &amp;= \argmax_{\Theta} E(\mathcal{X}; \Theta).\end{split}\]</div>
<p>This requires computing</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial E(\mathcal{X}; \Theta)}{\partial \Theta}
 &amp;= -\frac{\partial \log Z(\Theta)}{\partial \Theta} +
    \frac{1}{N} \sum_{i = i}^N
      \frac{\partial \log f(\mathbf{x}_i; \Theta)}{\partial \Theta}\\
 &amp;= -\left\langle
      \frac{\log f(\mathbf{x}; \Theta)}{\partial \Theta}
    \right\rangle_{p(\mathbf{D} \mid \Theta)} +
    \left\langle
      \frac{\log f(\mathbf{x}; \Theta)}{\partial \Theta}
    \right\rangle_{p(\mathbf{X})}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\mathbf{D} \mid \Theta)\)</span> represents the true underlying
distribution of the model, <span class="math notranslate nohighlight">\(p(\mathbf{X})\)</span> denotes the empirical data
distribution</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}) = \frac{1}{N} \sum_{i = 1}^N \delta(\mathbf{x} - \mathbf{x}_i),\]</div>
<p>and the derivative of the partition function is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial \log Z(\Theta)}{\partial \Theta}
 &amp;= \frac{1}{Z(\Theta)} \frac{\partial Z(\Theta)}{\partial \Theta}\\
 &amp;= \frac{1}{Z(\Theta)}
    \int_{\mathbf{x} \in \mathcal{D}}
      \frac{\partial f(\mathbf{x}; \Theta)}{\partial \Theta}
      \mathop{\mathrm{d}\mathbf{x}}\\
 &amp;= \frac{1}{Z(\Theta)}
    \int f(\mathbf{x}; \Theta)
      \frac{\partial \log f(\mathbf{x}; \Theta)}{\partial \Theta}
      \mathop{\mathrm{d}\mathbf{x}}\\
 &amp;= \int p(\mathbf{x} \mid \Theta)
      \frac{\partial \log f(\mathbf{x}; \Theta)}{\partial \Theta}
      \mathop{\mathrm{d}\mathbf{x}}\\
 &amp;= \left\langle
      \frac{\partial \log f(\mathbf{x}; \Theta)}{\partial \Theta}
    \right\rangle_{p(\mathbf{D} \mid \Theta)}.\end{split}\]</div>
</div>
<div class="section" id="relationship-between-maximum-likelihood-and-kullback-leibler-divergence">
<h3>Relationship between Maximum Likelihood and Kullback-Leibler Divergence<a class="headerlink" href="#relationship-between-maximum-likelihood-and-kullback-leibler-divergence" title="Permalink to this headline">¶</a></h3>
<p>Maximizing the log-likelihood of the data averaged over the data distribution
<span class="math notranslate nohighlight">\(Q^0\)</span> is equivalent to minimizing the relative entropy between the data
distribution and <span class="math notranslate nohighlight">\(Q^\infty\)</span>, the equilibrium distribution over the visible
variables that is produced by prolonged Gibbs sampling.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\entropy}{\mathit{H}}
\argmin_\theta Q^0 \parallel Q^\infty
 &amp;= \argmin_\Theta
      p(\mathbf{D}) \parallel p(\mathbf{D} \mid \Theta)\\
 &amp;= \argmin_\Theta \int_{\mathbf{d} \in \mathcal{D}} p(\mathbf{d})
      \log \frac{p(\mathbf{d})}{p(\mathbf{d} \mid \Theta)}
      \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \argmin_\Theta
      \int
        p(\mathbf{d}) \log p(\mathbf{d}) \mathop{\mathrm{d}\mathbf{d}} -
      \int
        p(\mathbf{d}) \log p(\mathbf{d} \mid \Theta)
        \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \argmin_\Theta -\entropy\left( Q^0 \right) -
      \left\langle \log p(\mathbf{d} \mid \Theta) \right\rangle_{Q^0}\\
 &amp;= \argmax_\Theta
      \left\langle \log p(\mathbf{d} \mid \Theta) \right\rangle_{Q^0}.\end{split}\]</div>
<p>The corresponding gradient is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial \theta_m} \left( Q^0 \parallel Q^\infty \right)
 &amp;= -\frac{\partial}{\partial \theta_m}
    \left\langle \log p(\mathbf{d} \mid \Theta) \right\rangle_{Q^0}\\
 &amp;= -\left\langle
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m} -
      \sum_\mathbf{c}
        p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
        \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \right\rangle_{Q^0}
    &amp; \quad &amp; \Theta = \theta_1, \ldots, \theta_n \text{ is a PoE}\\
 &amp;= -\int_{\mathbf{d} \in \mathcal{D}} p(\mathbf{d}) \left(
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m} -
      \sum_\mathbf{c}
        p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
        \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \right) \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \sum_\mathbf{c}
      p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
      \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \cdot
    \int p(\mathbf{d}) \mathop{\mathrm{d}\mathbf{d}} -
    \int p(\mathbf{d})
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m}
      \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \left\langle
      \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \right\rangle_{Q^\infty} -
    \left\langle
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m}
    \right\rangle_{Q^0}.\end{split}\]</div>
<p>Notice that when
<span class="math notranslate nohighlight">\(p(\mathbf{d}) = N^{-1} \sum_{i = 1}^N \delta(\mathbf{d} - \mathbf{d}_i)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\langle \log p(\mathbf{d} \mid \Theta) \right\rangle_{Q^0}
 &amp;= \int_{\mathbf{d} \in \mathcal{D}}
      p(\mathbf{d}) \log p(\mathbf{d} \mid \Theta)
      \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= N^{-1} \int \sum_{i = 1}^N
      \delta(\mathbf{d} - \mathbf{d}_i)
      \log p(\mathbf{d} \mid \Theta) \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \frac{1}{N} \sum_{i = 1}^N \log p(\mathbf{d}_i \mid \Theta).\end{split}\]</div>
</div>
<div class="section" id="contrastive-divergence">
<h3>Contrastive Divergence<a class="headerlink" href="#contrastive-divergence" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(Q^t\)</span> denote the data distribution of Gibbs sampling <span class="math notranslate nohighlight">\(Q^0\)</span> for
<span class="math notranslate nohighlight">\(t\)</span> full steps with <span class="math notranslate nohighlight">\(\lim_{t \to \infty} Q^t = Q^\infty\)</span>.</p>
<p>The corresponding gradient is now</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial \theta_m} \left( Q^t \parallel Q^\infty \right)
 &amp;= \frac{\partial}{\partial \theta_m}
    \int_{\mathbf{d} \in \mathcal{D}} Q(\mathbf{d} \mid \Theta)
      \log \frac{Q(\mathbf{d} \mid \Theta)}{p(\mathbf{d} \mid \Theta)}
      \mathop{\mathrm{d}\mathbf{d}}
    &amp; \quad &amp; Q^t \equiv Q(\mathbf{d} \mid \Theta)\\
 &amp;= \frac{\partial}{\partial \theta_m} \int
      Q(\mathbf{d} \mid \Theta) \log Q(\mathbf{d} \mid \Theta) -
      Q(\mathbf{d} \mid \Theta) \log p(\mathbf{d} \mid \Theta)
      \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \frac{\partial Q(\mathbf{d} \mid \Theta)}{\partial \theta_m}
        \frac{
          \partial \entropy\left( Q^t \right)
        }{
          \partial Q(\mathbf{d} \mid \Theta)
        } -
    \frac{\partial}{\partial \theta_m} \int
      Q(\mathbf{d} \mid \Theta) \log p(\mathbf{d} \mid \Theta)
      \mathop{\mathrm{d}\mathbf{d}}
    &amp; \quad &amp; \text{chain rule}\\
 &amp;= \frac{\partial Q(\mathbf{d} \mid \Theta)}{\partial \theta_m}
        \frac{
          \partial \entropy\left( Q^t \right)
        }{
          \partial Q(\mathbf{d} \mid \Theta)
        } -
    \int
      \frac{\partial Q(\mathbf{d} \mid \Theta)}{\partial \theta_m}
          \log p(\mathbf{d} \mid \Theta) +
      Q(\mathbf{d} \mid \Theta)
          \frac{\partial \log p(\mathbf{d} \mid \Theta)}{\partial \theta_m}
      \mathop{\mathrm{d}\mathbf{d}}
    &amp; \quad &amp; \text{product rule}\\
 &amp;= \frac{\partial Q(\mathbf{d} \mid \Theta)}{\partial \theta_m}
        \frac{
          \partial \entropy\left( Q^t \right)
        }{
          \partial Q(\mathbf{d} \mid \Theta)
        } -
    \frac{\partial Q(\mathbf{d} \mid \Theta)}{\partial \theta_m}
        \frac{
          \partial \left\langle
            \log p(\mathbf{d} \mid \Theta)
          \right\rangle_{Q^t}
        }{
          \partial Q(\mathbf{d} \mid \Theta)
        } -
    \int
      Q(\mathbf{d} \mid \Theta)
          \frac{\partial \log p(\mathbf{d} \mid \Theta)}{\partial \theta_m}
      \mathop{\mathrm{d}\mathbf{d}}
    &amp; \quad &amp; \text{chain rule}\\
 &amp;= \frac{\partial Q^t}{\partial \theta_m}
        \frac{\partial \left( Q^t \parallel Q^\infty \right)}{\partial Q^t} -
    \left\langle
      \frac{\partial \log p(\mathbf{d} \mid \Theta)}{\partial \theta_m}
    \right\rangle_{Q^t}\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\langle
  \frac{
    \partial \log p(\mathbf{d} \mid \Theta)
  }{\partial \theta_m}
\right\rangle_{Q^t}
 &amp;= \left\langle
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m} -
      \sum_\mathbf{c}
        p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
        \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \right\rangle_{Q^t}
    &amp; \quad &amp; \Theta = \theta_1, \ldots, \theta_n\\
 &amp;= \int_{\mathbf{d} \in \mathcal{D}} Q(\mathbf{d} \mid \Theta) \left(
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m} -
      \sum_\mathbf{c}
        p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
        \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \right) \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \int Q(\mathbf{d} \mid \Theta)
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m}
      \mathop{\mathrm{d}\mathbf{d}} -
    \sum_\mathbf{c}
      p(\mathbf{c} \mid \theta_1, \ldots, \theta_n)
      \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \cdot
    \int Q(\mathbf{d} \mid \Theta) \mathop{\mathrm{d}\mathbf{d}}\\
 &amp;= \left\langle
      \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m}
    \right\rangle_{Q^t} -
    \left\langle
      \frac{\partial \log p_m(\mathbf{c} \mid \theta_m)}{\partial \theta_m}
    \right\rangle_{Q^\infty}.\end{split}\]</div>
<p>The mathematical motivation behind contrastive divergence is the cancellation of
the intractable expectation over <span class="math notranslate nohighlight">\(Q^\infty\)</span>.  Since <span class="math notranslate nohighlight">\(Q^t\)</span> is
<span class="math notranslate nohighlight">\(t\)</span> steps closer to the equilibrium distribution <span class="math notranslate nohighlight">\(Q^\infty\)</span> than
<span class="math notranslate nohighlight">\(Q^0\)</span>, a reasonable gradient approximation that avoids sampling from the
<span class="math notranslate nohighlight">\(Q^\infty\)</span> is</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_m} \left(
  Q^0 \parallel Q^\infty - Q^t \parallel Q^\infty
\right) =
\left\langle
  \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m}
\right\rangle_{Q^t} -
  \left\langle
    \frac{\partial \log p_m(\mathbf{d} \mid \theta_m)}{\partial \theta_m}
  \right\rangle_{Q^0} -
  \frac{\partial Q^t}{\partial \theta_m} \frac{
    \partial \left( Q^t \parallel Q^\infty \right)
  }{\partial Q^t} \geq 0.\]</div>
<p>The last term on the right is typically ignored since computing it is
non-trivial and its contribution is negligible.  Note that contrastive
log-likelihood will fail because it can achieve a value of zero when all
possible vectors in the data space are equally probable.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-blog/2016/11/23/training-products-of-experts-by-minimizing-contrastive-divergence-0"><dl class="citation">
<dt class="bibtex label" id="hinton2002training"><span class="brackets"><a class="fn-backref" href="#id3">Hin02</a></span></dt>
<dd><p>Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. <em>Neural computation</em>, 14(8):1771–1800, 2002.</p>
</dd>
<dt class="bibtex label" id="hpcd"><span class="brackets"><a class="fn-backref" href="#id2">Puh</a></span></dt>
<dd><p>Helmut Puhr. Contrastive divergence. <span><a class="reference external" href="#"></a></span>http://www.igi.tugraz.at/lehre/SeminarE/SS10/puhr_E_2010.pdf. Accessed on 2017-05-12.</p>
</dd>
<dt class="bibtex label" id="ojwnocd"><span class="brackets"><a class="fn-backref" href="#id1">Woo</a></span></dt>
<dd><p>Oliver Woodford. Notes on contrastive divergence. <span><a class="reference external" href="#"></a></span>http://www.robots.ox.ac.uk/ ojw/files/NotesOnCD.pdf. Accessed on 2017-05-12.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/blog/2016/11/23/training-products-of-experts-by-minimizing-contrastive-divergence.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2013-2020, alphaXomega.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>